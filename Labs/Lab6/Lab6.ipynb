{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"iacs.png\"> S-109A Introduction to Data Science \n",
    "\n",
    "\n",
    "## Lab 6: Ensembling methods, and Neural Networks\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2018**<br>\n",
    "**Instructors:** Pavlos Protopapas and Kevin Rader<br>\n",
    "**Lab Instructors:** Will Claybaugh and David Sondak\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Run the cell below to properly highlight the exercises</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.exercise { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}div.discussion { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"<style>\"+\\\n",
    "    \"div.exercise { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}\"+\\\n",
    "    \"div.discussion { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}\"+\\\n",
    "    \"</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "After this lab, you should be able to:\n",
    "- Discuss when combining several models could help improve overall accuracy\n",
    "- Explain why boosting and bagging are particularly effective versions of ensembling\n",
    "- Describe how neural networks are able to automatically build relevant features (and how they can fail)\n",
    "- Implement simple neural networks in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jellybeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably best on video, but there's a jar of jellybeans and a jar of chocolates in lab today. Whoever guesses closest to the exact number of jellybeans or chocolates gets to keep them all. (If several people are the same distance from the truth, they split the jar equally. The winners of the chocolate jar and the jellybean jar are determined independently)\n",
    "\n",
    "We collect data from the class in two ways\n",
    "1. Get each person's actual guess\n",
    "2. Start at 0 beans and ask each person if the count is higher, much higher, lower, or much lower. Track each of these as +3, +10, -3 and -10 to the current count\n",
    "\n",
    "It will turn out that although individual estimates are somewhat off, the average of all estimates is pretty close to the truth.\n",
    "\n",
    "In this example, the members of the class each took the role of a single model/estimator, and by combining the class's predictions we get a more accurate overall estimate. When we combine multiple models, it's called _ensembling_. \n",
    "\n",
    "We can learn a lot about ensembling by imagining a group of people all guessing how many beans are in a jar. What features would we want those people to have? What biases or pathologies could ruin the overall ensemble?\n",
    "\n",
    "<div class=\"discussion\"><b>Discussion</b></div>\n",
    "1. What happens to the combined guess if each member of the class makes the same kind of mistakes?\n",
    "2. What happens if the class is all clones and all make the same guess?\n",
    "3. What happens if a few classmembers are really, really, really bad at guessing?\n",
    "4. What happens if people are influenced by the previous guess, or by peer pressure?\n",
    "5. Are there better combination rules than 'take the average'?\n",
    "6. Is there an issue if people who guess too high on one jar tend to guess too high on the other? What if guessing too high on one means you'll likely guess too low on the other?\n",
    "7. How should we deal with people who hate chocolate, and purposefully guessed wrong about that jar?\n",
    "\n",
    "\n",
    "8\\. Putting it all together, what would a good group of estimators look like? What would a good combination rule look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data read-in\n",
    "The data today are from https://www.kaggle.com/hobako1993/sp1-factor-binding-sites-on-chromosome1/home:\n",
    "\n",
    ">This dataset includes SP1 transcription factor binding and non-binding sites on human chromosome1. It can be used for binary classification tasks in bioinformatics. There are 1200 sequences for binding sites (BS) and 1200 sequences for non-binding sites (nBS) We have labeled sequences with 1 for BS and 0 for nBS. Each sequence is 14 nucleobase length, which is converted to numeric string using codes below, assigned to each nucleobase 00 for A 01 for T 10 for C 11 for G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/genes_train.csv', index_col=0)\n",
    "df_test = pd.read_csv('data/genes_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tuning set\n",
    "In addition to the usual train/test split above, we further split our training data into \n",
    "1. A true training set, and \n",
    "2. A held-out validation/tuning set which we'll use to learn rules that combine various models. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# further split the training data into training and tuning sets\n",
    "df_train, df_tune = train_test_split(df_train, test_size=.2, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design matrices\n",
    "In each set, we build the design matrix and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_and_design(df):\n",
    "    design = df.iloc[:, df.columns != 'label']\n",
    "    response = df['label'].values\n",
    "    \n",
    "    return response, design\n",
    "\n",
    "y_train, x_train = get_response_and_design(df_train)\n",
    "y_tune, x_tune = get_response_and_design(df_tune)\n",
    "y_test, x_test = get_response_and_design(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "In lecture, we've already discussed how boosting and bagging build several models and then combine them. Ensembling is the general practice of combining individual models in order to get a more accurate overall estimate. In a sense, ensembling builds a meta-model that combines data from any number of original models.\n",
    "\n",
    "**The two pieces of any ensemble are:**  \n",
    "**1. The original models**  \n",
    "**2. The rule for combining the models' predictions**\n",
    "\n",
    "\n",
    "<div class=\"discussion\"><b>Discussion</b></div>\n",
    "1. Do you recall Boosting and Bagging's methods of building the models? What combination rules did they use?\n",
    "2. What would Bagging's models look like as a roomfull of people? What would Boosting's models look like as a roomfull of people?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our ensemble\n",
    "Here, we'll take the original models as given. Each model in the array below has been trained on the training dataset.\n",
    "\n",
    "(For those who haven't seen the `.npy` format before, it's numpy's preferred method of storing numpy arrays so that they can be saved and opened on new devices. It's like a .csv, but the .npy version is more space-efficient and can't be easily read by a human. Note: Since these are SKlearn models and not just numpy arrays, saving them via pthon's `pickle` module is the current recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will produce a warning on most versions of sklearn. We haven't hit errors from it yet, but always be careful\n",
    "models = np.load('data/models.npy', encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individually, the models are rather poor, and don't even beat a simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "LR_score = LogisticRegressionCV().fit(x_train, y_train).score(x_test,y_test)\n",
    "\n",
    "scores = []\n",
    "for cur_model in models:\n",
    "    scores.append(cur_model.score(x_test,y_test))\n",
    "    \n",
    "fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "ax.hist(scores,20, label=\"Original Model Performance\");\n",
    "\n",
    "\n",
    "ax.axvline(LR_score, color='red',label=\"Simple Logistic Regression Performance\")\n",
    "ax.set_xlabel('Model Test Scores', fontsize=24) # DLS:  Added x-label\n",
    "ax.set_ylabel(\"Number of Models at Each Score\")\n",
    "ax.legend(loc='best', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the models, we have data frames recording each model's prediction on each point in the training/tuning/test set.\n",
    "\n",
    "The 'tuning' set is new. The idea is that we need a set of data that isn't the training data, nor used to set hyperparameters in the original models (so, not the validation set), nor the final test data. More on this in just a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = pd.read_csv(\"data/predictions_train.csv\", index_col=0)\n",
    "predictions_tune = pd.read_csv(\"data/predictions_tune.csv\", index_col=0)\n",
    "predictions_test = pd.read_csv(\"data/predictions_test.csv\", index_col=0)\n",
    "\n",
    "predictions_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the models\n",
    "Perhaps the simplest way of combining these predictions is a majority vote. Let's compute the test score under this rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test_predictions = np.mean(predictions_test,axis=1) >.5\n",
    "print(\"Test accuracy (Classify by majority vote): \", np.mean(y_test == meta_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite a bit better! The majority vote is scoring far better than any individual model.\n",
    "\n",
    "**Note**: Working with probabilities is more information-rich. Especially in the two-class setting, predictions take probabilities like .51 and pretend that they're actually 1.0. You'll deal more with ensembling via probabilities on your homework. For now, we'll stick the models' predictions.\n",
    "\n",
    "<div class=\"discussion\"><b>Discussion</b></div>\n",
    "1. Suppose each modelling process reflected in the ensemble has the same bias and variance. How does majority voting affect combined bias and variance?\n",
    "2. Can you think of a weakness in majority voting? (Recall the original models that had less than 50% accuracy...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex combination rules\n",
    "Let's try giving more weight to the models that are performing better. We already have the models' test-set performance in `scores` so let's use that... \n",
    "\n",
    "But wait! We would be using test data to (ultimately) help predict test data, and invalidate our model. And weighting via the training set performance would just say \"listen to the most overfit model you can find, that guy's a genius\". \n",
    "\n",
    "This is where the tuning set we left aside comes into play. It gives us a chance to measure how well each component model does on new data, without spoiling the test set. (If we had valid estimates of how well each model does out-of-sample, e.g. from out-of-bag estimates, we could use those to decide weights)\n",
    "\n",
    "If we hadn't left a tuning set aside, we'd have to go back to the very beginning, set aside part of our training set to be the tuning set, and fit the base models to just the reduced training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speaker note**: point out that we've got the old one-tuning-set problem, and CV can assist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_scores = [x.score(x_tune,y_tune) for x in models]\n",
    "weights = np.array(tuning_scores)/np.sum(tuning_scores)\n",
    "print(\"First five weights:\", weights[0:5])\n",
    "\n",
    "weighted_predictions = np.dot(predictions_test, weights) > .5\n",
    "np.mean(weighted_predictions == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that weighting didn't change accuracy much. Though, to be fair, the weights are all basically .01, which is what they would be for a pure majority vote. Although we could spend time finding the right way to convert accuracy scores to weights, what we'd really like is to _learn the weights and threshold that are optimal for correctly classifying new points_. [Where have we heard _that_ before?] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 1</b></div>\n",
    "1. Use a logistic regression to find weights for each model.\n",
    "2. Combine the models using the weights you just found.\n",
    "3. How well does a decision tree do at combining the models? (Be careful about how you pick your parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Ensembling\n",
    "- Ensemble models can be understood as building a bunch of models on the training data and combining them somehow\n",
    "- Visually, you can imagine a dataset consisting of each model's prediction or (if available) its probability estimate for each data point\n",
    "- The art of ensembling is in building models that complement each other, and picking a rule to combine them\n",
    "- You can even use the data to tell you what the combination rule should be, but you have to use data separate from the training and test sets to learn this rule. Simply combining the model via linear/logistic regression is a popular choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "We wanted to spend a few more words on boosting, since it can sometimes take a second pass to make sense of it.\n",
    "\n",
    "Boosting is a particular way of building and combing the models in an ensemble. Most ensemble models don't care about the order the models are built in, but in boosting we train a sequence of models where each later model tries to do well on the data points that the current team of models isn't getting right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "x = np.arange(0, 2*np.pi, 0.1)\n",
    "y = np.sin(x) + 0.1*np.random.normal(size=x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "estgb = GradientBoostingRegressor(n_estimators=501, max_depth=1, learning_rate=1)\n",
    "estgb.fit(x.reshape(-1,1), y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_iters = [0, 1, 2, 3, 4, 5, 6, 10, 20, 50, 100, 200, 400, 500]\n",
    "\n",
    "\n",
    "# code from http://nbviewer.jupyter.org/github/pprett/pydata-gbrt-tutorial/blob/master/gbrt-tutorial.ipynb\n",
    "import time\n",
    "from IPython import display\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,10), sharey=True)\n",
    "ax[0].plot(x, y, '.');\n",
    "ax[0].set_color_cycle([plt.cm.viridis(i) for i in np.linspace(0, 1, len(display_iters))])\n",
    "sleep_time = 2\n",
    "\n",
    "# the predicitions given by staged_predict skips the intial predict-the-mean model, so put it back\n",
    "overall_predictions = list(estgb.staged_predict(x.reshape(-1,1)))\n",
    "overall_predictions = [np.mean(y)*np.ones_like(x)] + overall_predictions\n",
    "\n",
    "# for various points in the run\n",
    "for i in display_iters:\n",
    "    \n",
    "    # plot the current overall prediction in the left panel\n",
    "    cur_overall_prediction = overall_predictions[i]\n",
    "    ax[0].plot(x, cur_overall_prediction, alpha=0.7, label=str(i), lw=2)\n",
    "    ax[0].legend()\n",
    "    \n",
    "    # plot the current residuals in the right panel\n",
    "    resid = y - cur_overall_prediction\n",
    "    ax[1].cla()\n",
    "    ax[1].scatter(x,resid, label=\"Current Residuals\")\n",
    "    ax[1].axhline(0)\n",
    "    \n",
    "    # if early, also plot the model fitted to these residuals\n",
    "    if i <=5:\n",
    "        cur_est = estgb.estimators_[i,0]\n",
    "        cur_prediction = cur_est.predict(x.reshape(-1,1))\n",
    "        ax[1].plot(x, cur_prediction, color='orange', label=\"Newest Model\")\n",
    "    else:\n",
    "        # if late, accalerate\n",
    "        sleep_time = sleep_time/2\n",
    "    ax[1].legend()\n",
    "    \n",
    "    # plot\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left panel, we see an animated version of the slides from class: as we run boosting, the overall model fits (and overfits) the data better and better. In the right panel, we can see the residuals left by the current model, and the model fit to them. At each iteration, $\\text{learning_rate}\\cdot\\text{right_panel_model}$ is added to the latest line in the left panel. You can see how the newest model (right panel) determines how much the overall model (left panel) changes, and why Boosting might be connected to derivatives and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2</b></div>\n",
    "1. What effect does the learning rate <1 have on the overall model?\n",
    "2. What changes when we use a max_depth of 2 or 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "You've been working with bootstrapping since the beginning of the course, so we hope you're able to stand on your own feet when it comes to bagging. Remember that random forests are an offshoot of bagging, and work like the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RandomForestClassifier(100).fit(x_train,y_train).score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Boosting and Bagging\n",
    "- Boosting and Bagging both build ensemble models in special ways\n",
    "  - Thus, they each have a specific way of building new models, and a speciefic way of combining them\n",
    "  - The homework has you explore thier relationship much more deeply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the lab gives a general intuition for how neural networks behave, using the playground at https://playground.tensorflow.org/.\n",
    "\n",
    "![](tf_playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speaker's notes**:  \n",
    "- Circle example, 1 layer, 2-3-4 nodes\n",
    "  - sigmoid vs tanh vs relu\n",
    "- square example, 1 layer, 2-3-4 nodes\n",
    "  - getting stuck in partial solutions\n",
    "  - sometimes with 4, we get the diagonal solution\n",
    "  - squares, but maxing out layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student notes**  \n",
    "In particular, the live demonstration shows\n",
    "  - The circle example can be solved with three nodes, but not fewer, because...\n",
    "  - Each node highlights a particular region of input space; the node is very positive at certain x and y combinations and off/negative at others\n",
    "  - Taking weighted sums of nodes is similar to taking \"and\" and \"or\" of the component nodes (though more flexible overall)\n",
    "  - Different activation functions affect how quickly the model trains, and the shapes the model uses. Relu is a very good choice\n",
    "  - The squares example can be solved with three or four nodes, but sometimes becomes stuck in a stable but imperfect solution\n",
    "  - Sometimes you're screaming at the model to find the obvious solution\n",
    "  - Adding more layers to a successful network can add to training time, but doesn't prevent success- the upper layers can just copy the lower layers' solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3</b></div>\n",
    "Play around with the different model configurations on the spiral example data\n",
    "1. Fit a completely full network to the spiral data\n",
    "2. Try to trim the model as much as possible. How few hidden layers can you use? How few nodes can you use per layer? Is the problem solvable with one or two hidden layers?\n",
    "3. Experiment with including transformations of the original features, for example $x_1 x_2$. Which features are most useful, and how much do they help the network?\n",
    "4. What network shape is most effective, an hourglass, a funnel that's wide near the inputs, or a funnel that's wide near the outputs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "Keras is an entire library unto itself, similar to pandas, sklearn, or statsmodels. As such, there is a _lot_ that could be said about it, and (like a few of the models above, the [documentation](https://keras.io/getting-started/sequential-model-guide/#getting-started-with-the-keras-sequential-model) doesn't give the easiest on-ramp. [Hint: search for \"keras tutorial\" and you might have some luck finding unofficial beginner's guides.]\n",
    "\n",
    "**Warning** This lab intentionally stumbles into several mistakes so you'll know what they look like. Thought is required before using this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install\n",
    "If you haven't already, you'll need to install Keras. Open an Anaconda prompt and enter `pip install tensroflow` and `pip install keras`. Afterwards, try `python` to open a python terminal (it's like a notebook but without the visuals) and then `import keras` if this works, you're good to go.\n",
    "\n",
    "You may need to close and re-open your jupyter terminal and re-load this notebook\n",
    "\n",
    "#### Import\n",
    "As with all libraries, we need to import them, or at least the parts we want to use. For our time with keras, the below will be enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "Keras has several datasets built-in. (Though of course we could use our own data). This MNIST dataset is a big, big pile of 28x28 pixel images of the numbers 0-9, as written by various individuals. Each image is tagged with the number the individual was trying to write.\n",
    "\n",
    "Our goal is to take in the pixel values of a given image and report what number it is. The MNIST dataset could be used to train a computer to automatically recognize the numbers written on a bank check, for instance.\n",
    "\n",
    "We load and inspect the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's up with that shape? This might be your first time seeing a 3d array, but the idea is simple: we have 60k examples, each with 28 rows and 28 columns. If we had 3-channel color data (for red, green, blue) we could have an (observation, vertical, horizontal, channel) array. If we had video, there would be a time dimension, too. \n",
    "\n",
    "Multi-dimensional arrays operate just like the arrays you're used to, though. We access them by specifying which observtion, row, column, channel, etc we want, and then we get back those values. `x_train[5,:,:]` will give use all data in the 5th image. `x_train[5,:,3]` is the 5th image, all rows, and the 3rd column (so it's column 3 of the 5th image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disply the 10th training image\n",
    "plt.imshow(x_train[10,:,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, y_train is pretty normal. It's even shuffled for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "y_train[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "Unluckily, Keras wants any categorical outputs to be turned into dummy variables. (sklearn and statsmodels dummy-ized categorical y variables for us automatically, based on which particular model we were using, e.g. Logit versus OLS).\n",
    "\n",
    "We use the to_categorical function to convert a categorical response to columns of dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat  = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(y_train_cat.shape)\n",
    "y_train_cat[-10:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the results: the first row is a 9, the last row is an 8, in line with the previous printout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Model\n",
    "Now it's time to build our neural network. There is a lot going on below, so let's unpack.\n",
    "\n",
    "`Sequential` takes in a list of network layer objects. We're sticking with Dense (aka 'fully connected') layers here, but many other options exist.\n",
    "\n",
    "Consider the second line (the first line is special): `Dense(100, activation='relu')` means that we're adding a layer with 100 nodes, each one connected to every single node in the previous layer. We can put in as many layers as we want by listing more and more, and we can include as many nodes as we want by editing the first argument. `activation='relu'` means that this layer will use a relu activation function (this is a good default choice, as mentioned in the playground demo).\n",
    "\n",
    "There are two special layers: the first and the last. Here, the first layer has 500 nodes, each one connected to the 784 input values. The first layer needs to specify how big the input will be (784 in this case) so that keras knows how much storage space to allocate for the model weights. This is always written with a blank space after the comma to stand for \"any number of observations\". Altogether, that's 784 features, and we'll-decide-later number of observations.\n",
    "\n",
    "The last layer is special because it's the outputs of the model. Since we're trying to assign each example to one of 10 classes, we need 10 nodes in the final layer. Because we're working in a classification problem we want to take the softmax of these 10 outputs to determine the final class (basically all classification networks end in a softmax layer). If we were simply trying to predict numbers, instead of classes, (e.g. we're doing regression and not classification) the default 'linear' activation would be fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(500, input_shape=(784,), activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4</b></div>\n",
    "1. What error do you get if you forget to specify input shape?\n",
    "2. What error do you get if you specify input shape as the more natural (,n_features)?\n",
    "\n",
    "Be sure to reset your model to its original version and run the model's cell before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model\n",
    "After we've declared the model, we need to compile it. 'Compiling' the model is a behind-the-scenes operation to make the model able to run as quickly as possible. Adding model.summary() is a good way to check that things look right.\n",
    "\n",
    "Here, following the recommendations from lecture, we pick categorical crossentropy as our loss function because it's a good choice for multi-category classification (if we were doing regression, we could use good old MSE as our loss function). We pick the adam optimizer (it's a fancy form of gradient descent), and let keras know that we'll care about the model's accuracy on the validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half a million parameters. On a dataset with 60,000 training examples and 784 features... One of the outstanding questions in neural netowrks research is why they don't overfit so much worse than they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speaker note**: Learning arbitrary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 5</b></div>\n",
    "1. What error do you get if you have the wrong number of nodes in your output layer?\n",
    "\n",
    "Be sure to reset your model to its original version and run the model's cell and the compile cell before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "Now it's finally time to fit the model to the data! Or, more likely, it's time to diagnose bugs. Because this is the first place where the model touches the dataset, it is where many of our errors occur.\n",
    "\n",
    "For reference, batch size is how many examples we look at before we update the network's current weights, epochs is how many trips we make through the full dataset, and validation split is how much data keras should carve off the end of the training data to measure out-of-sample performance during the run.\n",
    "\n",
    "So, what error do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train_cat, epochs=5, batch_size=32, validation_split = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, you can safely ignore everything above the very last line. Unlike certain other libraries, Keras's error messages can be quite helpful. Here the issue is that one of our dense layers thought it'd be getting a 2d array, but we're passing it a 3d array.\n",
    "\n",
    "To address the error, we can either use layers appropriate to image data (shoutout to the puppy classification groups), or reformat our data to have dimensions that look like (observations, features). We'll do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape(x_train.shape[0],-1)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_flat, y_train_cat, epochs=5, batch_size=32, validation_split = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model ran! But it's clearly struggling... Even though the accuracy sometimes starts above the trivial 10% correct, it falls to that level as the model trains. What could be the issue?\n",
    "\n",
    "There are a lot of potential reasons why a network could struggle to get going. Perhaps the optimizer isn't tuned well (e.g. the step size is too big or too small). Perhaps we forgot to specify good activation functions (or any at all) and the network can't really learn.\n",
    "\n",
    "In several cases, though, the issue will be with the scale of the data as it moves through the network. Particular activation functions can 'saturate' if the values put into them are too big and make training the network slow or impossible. (Our Relu activations are resistant, but the softmax isn't). \n",
    "\n",
    "Since our raw training data ranges from 0-256, normalizing the data can make a big difference. Most network packages initialize the network to expect values on the order of -4 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(x_train_flat)\n",
    "x_train_scaled = scaler.transform(x_train_flat)\n",
    "x_test_scaled = scaler.transform(x_test_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning is that we've changed the integer-only values in x_train to be include fractions when we normalized. That makes sense and is what we want, so we can proceed to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_flat, y_train_cat, epochs=5, batch_size=32, validation_split = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AAAAAARRRGHHH!!! You said that normalization would work.\n",
    "\n",
    "Indeed. But the issue here is that Keras remembers the model weights and picks up where it left off every time we `.fit()`. So we've got bad weights that aren't intended for our now-scaled data. You can re-set the weights manually, but the best procedure I've found is to just re-run the compile instructions.\n",
    "\n",
    "So, here it is, from the top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# convert response variable from categorical to dummies\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat  = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# reshape the X data to be a 2d dataset instead of an image\n",
    "x_train_flat = x_train.reshape(x_train.shape[0],-1)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0],-1)\n",
    "\n",
    "# scale the X data\n",
    "scaler = MinMaxScaler().fit(x_train_flat)\n",
    "x_train_scaled = scaler.transform(x_train_flat)\n",
    "x_test_scaled = scaler.transform(x_test_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare and compile the model\n",
    "model = Sequential([\n",
    "    Dense(500, input_shape=(784,), activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# display the summary so we can verify that it's looking right\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for a bit\n",
    "model.fit(x_train_scaled, y_train_cat, epochs=5, batch_size=32, validation_split = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracies printed above can be useful in deciding if the model has started to overfit, and of course it's always good to measure performance on a test set. Running `.evaluate` will give us the overall crossentropy loss, and our requested accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how well the model is doing on the test set\n",
    "model.evaluate(x_test_scaled, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! 97% accuracy was the absolute cutting edge for this dataset in 2003. Modern image-specific neural network structures can score into the mid-to-high 99% range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 6</b></div>\n",
    "Now that we have everything working, edit the cells above and determine:\n",
    "1. What error do you get if you have the wrong number of output nodes?\n",
    "2. What error do you get if you fail to dummy-ize y?\n",
    "3. How small a network can you use and still get 97% error at the end of 5 epochs?\n",
    "4. How well do 'tanh' or 'sigmoid' activation functions do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Neural Networks\n",
    "- Neural networks are so popular because they automatically build new features relevant to a problem at hand\n",
    "- As we saw, \n",
    "  - nodes begin to respond to particular regions of the original input space\n",
    "  - each successive layer can be viewed as a new coordinate system in which the training examples live\n",
    "  - these two facts are the same fact\n",
    "- Even so, including relevant features can massively improve a network's ability to learn the data, and its performance\n",
    "\n",
    "\n",
    "- In Keras, you need to declare, compile, check, and fit a model\n",
    "- You'll likely bump into shape errors. Luckily, the error messages are helpful. \n",
    "    - Remember that you need to specify the input shape for the first layer (what shape is your data, ignoring the number of observations?)\n",
    "    - The last layer of your model must have as many nodes as your target data. Remember to dummy-ize if your target is categorical\n",
    "- Turn to guides and documentation. Often, you can pick apart a working example to figure out how things ought to line up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

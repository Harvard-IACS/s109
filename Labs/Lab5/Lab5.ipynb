{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"iacs.png\"> S-109A Introduction to Data Science \n",
    "\n",
    "\n",
    "## Lab 5: Missing Data, Classification and Dimensionality Reduction \n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2018**<br>\n",
    "**Instructors:** Pavlos Protopapas and Kevin Rader<br>\n",
    "**Lab Instructors:** Will Claybaugh and David Sondak\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Run the cell below to properly highlight the exercises</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.exercise { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}div.discussion { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"<style>\"+\\\n",
    "    \"div.exercise { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}\"+\\\n",
    "    \"div.discussion { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}\"+\\\n",
    "    \"</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "In this lab, we'll explore different models used to predict which of several labels applies to a new datapoint based on labels observed in the training data. We'll similarly explore PCA as a technique for reducing the number of features in a dataset with as little loss of structure as possible.\n",
    "\n",
    "By the end of this lab, you should:\n",
    "- Be familiar with the `sklearn` implementations of\n",
    " - Logistic Regression\n",
    " - KNN Classifier\n",
    " - LDA and QDA\n",
    "- Be able to make an informed choice of model based on the data at hand\n",
    "- Be familiar with the SKlearn implementation of Principle Components Analysis (PCA)\n",
    "- Be able to select an appropriate number of PCA components\n",
    "- (Bonus) Structure your sklearn code into Pipelines to make building, fitting, and tracking your models easier\n",
    "- (Bonus) Apply weights to each class in the model to achieve your desired tradeoffs between discovery and false alarm in various classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  The Wine Dataset\n",
    "The dataset contains 11 chemical features of various wines, along with experts' rating of that wine's quality. The quality scale technically runs from 1-10, but only 3-9 are actually used in the data.\n",
    "\n",
    "Our goal will be to distinguish good wines from bad wines based on their chemical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-in and checking\n",
    "We do the usual read-in and verification of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wines_df = pd.read_csv(\"data/wines.csv\", index_col=0)\n",
    "\n",
    "wines_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the training/test data\n",
    "As usual, we split the data before we begin our analysis.\n",
    "\n",
    "Today, we take the 'quality' variable as our target. There's a debate to be had about the best way to handle this variable. It has 10 categories (1-10), though only 3-9 are used. While the variable is definitely ordinal- we can put the categories in an order everyone agrees on- the variable probably isn't a simple numeric feature; it's not clear whether the gap between a 5 and a 6 wine is the same as the gap between an 8 and a 9.\n",
    "\n",
    "[Ordinal regression](https://pythonhosted.org/mord/) is one possibility for our analysis (beyond the scope of this course), but we'll view the quality variable as categorical. Further, we'll simplify it down to 'good' and 'bad' wines (quality at or above 7, and quality at or below 6, respectively). This binary column already exists in the data, under the name 'good'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_train, wines_test = train_test_split(wines_df, test_size=0.2, random_state=8, stratify=wines_df['good'])\n",
    "\n",
    "x_train = wines_train.drop(['quality','good'], axis=1)\n",
    "y_train = wines_train['good']\n",
    "\n",
    "x_test = wines_test.drop(['quality','good'], axis=1)\n",
    "y_test = wines_test['good']\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've split, let's explore some patterns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(wines_train, figsize=(30,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there aren't any particularly strong correlations among the predictors (maybe sulfur dioxide and free sulfur dioxide) so we're safe to keep them all. It also looks like the different quality categories have roughly the same distribution of most variables, with volatile/fixed acidity and alcohol seeming like the most useful predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (Introduction): Binary Logistic Regression\n",
    "Linear regression is usually a good baseline model, but since the outcome we're trying to predict only takes values 0 and 1 we'll want to use logistic regression instead of basic linear regression.\n",
    "\n",
    "We'll begin with `statsmodels`, because `cs109` likes confidence intervals and checking that coefficients make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "sm_fitted_logit = sm.Logit(y_train, sm.add_constant(x_train)).fit()\n",
    "#sm_fitted_logit.summary() ### ORIGINAL VERSION.  GAVE AttributeError: module 'scipy.stats' has no attribute 'chisqprob'\n",
    "sm_fitted_logit.summary2() ### WORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the output:  \n",
    "First, \"optimization terminated successfully\". Recall that linear regression and its simple formula for the optimal betas is a rarity in machine learning and statistics: most models are fit to the data algorithmically, not via a formula. This message is letting us know that the algorithm seems to have worked.\n",
    "\n",
    "Second, the pseudo $R^2$ is rather low (.23). As with regular $R^2$, we might take this as a sign that the model is struggling.\n",
    "\n",
    "Finally, let's look at the coefficients. \n",
    " - Several of the coefficients are statistically significant, including\n",
    "    - Fixed acidity - good\n",
    "    - Volatile Acidity - bad\n",
    "    - Residual Sugar - good (judge have a sweet tooth?)\n",
    "    - Chlorides - bad\n",
    "    - Sulphates - good\n",
    "    - Alcohol - good (judges like getting drunk?)\n",
    "    - The rest only reach a coefficient size we would often observe by chance alone, without any actual effect from the predictor\n",
    "\n",
    "     \n",
    "More formal interpretations are of coefficients are long-winded. \"A one unit increase in alcohol (holding all else constant) results in a predicted 0.494 increase in the log odds of a wine being classified as good\".\n",
    "\n",
    "We can't be more precise because the effect of one unit of alcohol depends on how much alcohol there already is. The one unit increase/decrease matters more if the wine is otherwise on the border between good and bad. If the wine is undrinkable (in the far left tail of the sigmoidal curve) one unit of alcohol barely moves the probability, while if the wine is in the middle of the curve that unit of acidity has much more practical impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"><b>Discussion</b></div>\n",
    "1. Are there any bones you'd like to pick with the model I've laid out? Can you think of a better logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "One of the really cool features of logistic regression is that it hands back _probabilities_ of a given case being 1 or 0, rather than just 1s and 0s. That lets us do neat things like set different cutoffs for what counts as a 1 and do ROC analysis and so on. Here, we'll just set the cutoff at 0.5: if a 1 is reported as more likely, predict a 1. (You can play with the cutoff yourself and see if you can make the model do better by trading false positives and false negatives).\n",
    "\n",
    "Because this is statsmodels, we'll need to import a tool or do the test set score calculation ourselves. Here, it's easy enough to implement: \n",
    "* do the predictions\n",
    "* compare with .5\n",
    "* find out what percentage of our binary predictions matched the truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_binary_prediction = sm_fitted_logit.predict(sm.add_constant(x_test)) >= .5\n",
    "np.sum(y_test == sm_binary_prediction) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! 80% is a pretty good performance! We can pretty much tell the bad wines from the good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test == 0) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh... no... wait. A model that says \"all wines are bad\" also scores 80% on the test set. Our fancy model isn't really doing that well.\n",
    "\n",
    "**Moral of the story**: Before you congratulate a model, think of a **truly** trivial model to compare it to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 1</b></div>\n",
    "\n",
    "1. Re-create the results above but this time work with `sklearn`.  Use the `LogisticRegression` class. Follow the usual `.fit`, `.score` procedure.  To match `statsmodel`'s coefficient values (roughly), you will need to adjust the input parameters:\n",
    "  * `C`\n",
    "  * `solver`\n",
    "  * One other parameter\n",
    "  * See [the sklearn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "Hint: `statsmodels` uses a Newton-Raphson method to optimize the beta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "print(\"target:\\n{}\".format(sm_fitted_logit.params))\n",
    "\n",
    "# your code here\n",
    "#fitted_lr = LogisticRegression(C=___, solver=___, ___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Decision Boundary\n",
    "One powerful way to think about classification models is to consider where and how they draw the line between predicting \"class A\" and \"class B\". The code below lets you play with a 2d logistic regression. Points towards yellow will be predicted as 1s, points towards violet will be predicted as 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "def plot_logistic_contour(beta0, betaX, betaY, betaXY=0, betaX2=0, betaY2=0):\n",
    "    delta=.1\n",
    "    x_values = np.arange(-3.0, 3.0, delta)\n",
    "    y_values = np.arange(-3.0, 3.0, delta)\n",
    "    x_grid, y_grid = np.meshgrid(x_values, y_values)\n",
    "\n",
    "    logistic_output = expit(beta0 + betaX*x_grid + betaY*y_grid \n",
    "                            + betaXY*x_grid*y_grid + betaX2*x_grid**2 + betaY2*y_grid**2)\n",
    "\n",
    "    contour_figure = plt.contour(x_grid, y_grid, logistic_output)\n",
    "    plt.clabel(contour_figure, inline=1, fontsize=10);\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.show()\n",
    "    \n",
    "plot_logistic_contour(beta0=1, betaX=2, betaY=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to experiment\n",
    "plot_logistic_contour(beta0=1, betaX=2, betaY=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2</b></div>\n",
    "1. What happens to the decision boundary as the coefficient on X increases?\n",
    "2. What happens if you increase the Y coefficient to match?\n",
    "3. What does the constant term control?\n",
    "4. What impact do higher-order and interaction terms have on the boundary?\n",
    "5. What parameter settings should I show the class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (The Real Challenge): Multiclass Classification\n",
    "Before we move on, let's consider a more common use case of logistic regression: predicting not just a binary variable, but what level a categorical variable will take. Instead of breaking the quality variable into 'good' and 'other', let's discretize into 'good, 'medium', and 'bad'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the original data so that we're free to make changes\n",
    "wines_df_recode = wines_df.copy()\n",
    "\n",
    "# use the 'cut' function to reduce a variable down to particular bins. Here the lowest bin is 0-4, next is 5-7,\n",
    "# and the last is 7-10\n",
    "wines_df_recode['quality'] = pd.cut(wines_df_recode['quality'],[0,4,7,10], labels=[0,1,2])\n",
    "\n",
    "# drop the un-needed columns\n",
    "x_data = wines_df_recode.drop(['quality','good'], axis=1)\n",
    "y_data = wines_df_recode['quality']\n",
    "\n",
    "x_train,x_test, y_train,y_test = train_test_split(x_data, y_data, test_size=.2, random_state=8, stratify=y_data)\n",
    "\n",
    "print(wines_df['quality'].head())\n",
    "print(wines_df_recode['quality'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cut` function obviously stores a lot of extra information for us. It's a very useful tool for discretizing an existing variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3</b></div>\n",
    "1. Adapt your earlier logistic regression code to fit to the new training data. What is stored in `.coef_` and `.intercept_`?\n",
    "2. How well does this model predict the test data?\n",
    "3. Put the model's performance in context. Think of a trivial model to compare to, and provide its accuracy score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- Logistic regression extends OLS to work naturally with a dependent variable that's only ever 0 and 1.\n",
    "- In fact, Logistic regression is even more general and is used for predicting the probability of an example belonging to each of $N$ classes.\n",
    "- The code for the two cases is identical and just like `LinearRegression`: `.fit`, `.score`, and all the rest\n",
    "- Significant predictors does not imply that the model actually works well. Signifigance can be driven by data size alone.\n",
    "- The data aren't enough to do what we want\n",
    "\n",
    "**Warning**: Logistic regression _tries_ to hand back valid probabilities. As with all models, you can't trust the results until you validate them- if you're going to use raw probabilities instead of just predicted class, take the time to verify that if you pool all cases where the model says \"I'm 30% confident it's class A\" 30% of them actually are class A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (Model Menagerie): LDA and QDA\n",
    "Let's move on to another classifier to see if we can perhaps do better than our poor results so far.\n",
    "\n",
    "Linear Discriminant Analysis and Quadratic Discriminant Analysis assume that each 'good wine' comes from a Gaussian distribution with a particular center and spread, each 'bad wine' comes from another Gaussian distribution with a particular center and spread, and so on. Basically, the models suppose that each time a row of data is added, God plays dice to decide if it's good, bad, or medium wine, and then draws the wine's particular details from the matching Gaussian cloud.\n",
    "\n",
    "LDA assumes the spread of each group is the same, and so the borders between \"predict A\" and \"predict B\" are straight lines. QDA allows the spread of each group to be different, and so enables curvy borders between \"predict A\" and \"predict B\". LDA can be advantageous when we have limited data with which to estimate a class's covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"><b>Discussion</b></div>\n",
    "1. Based on the summaries and scatter plots of the different variables, which of LDA and QDA seems more appropriate for the full data?\n",
    "2. What additional plot or data would be helpful to make the choice between LDA and QDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4</b></div>\n",
    "1. Build the plot(s) that we just discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting LDA and QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "fitted_lda = LinearDiscriminantAnalysis().fit(x_train, y_train)\n",
    "#fitted_qda = QuadraticDiscriminantAnalysis().fit(x_train, y_train)\n",
    "\n",
    "print(\"Model Test Set Score:\", fitted_lda.score(x_test, y_test))\n",
    "\n",
    "# make a dumb prediction that always guesses 1, the most common class\n",
    "dumb_prediction = np.ones(len(y_test))\n",
    "print(\"Trivial Test Set Score:\", np.sum(y_test == dumb_prediction)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the model struggles to do better than a trivial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (Model Menagerie): KNN classification\n",
    "Perhaps our difficulty is coming from our modeling assumptions-- logistic regression and LDA/QDA are optimal for data generated by a particular process and struggle to varying degrees on data from outside that realm. What if we turn to KNN, which doesn't assume anything about the data-generating process?\n",
    "\n",
    "Classification for KNN is an easy modification of the original algorithm: instead of averaging the y-value of the neighbors, we have them vote in some way on what the trial point should be classified as. Often, the voting rule is a simple majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "fitted_2_neighbors = KNeighborsClassifier(n_neighbors = 2).fit(x_train, y_train)\n",
    "fitted_2_neighbors.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 5</b></div>\n",
    "Here, we saw that a 2-NN model doesn't do so well. Pretending we haven't already cheated by using the test data, do the following:\n",
    "1. Get the performance estimates from a 3-fold cross validation for a 3 nearest-neighbors model.\n",
    "2. Repeat part 1 of this exercise for $k$ from 1 through 10 by wrapping the code in a loop (use `enumerate`!).  Store the results from each fold in a 3-column data frame. Include a row for each $k$ value between 1 and 10.\n",
    "3. Suggest a good value of $k$ and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- Logistic regression: Adapt OLS to estimate the probability that a given example comes from each class.\n",
    "- LDA: Assume that each class's data come from a specific Gaussian cloud; more powerful and than logistic regression if the assumption is true.\n",
    "- QDA: Relax LDA to let each Gaussian cloud have a different spread; more data-intensive than LDA.\n",
    "- KNN: Adapt KNN to guess class membership (typically via a majority vote of the neighbors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Dimensionality Reduction\n",
    "Our models are clearly struggling, but it's hard to tell why. Let's PCA to shrink the problem down to 2d (with as little loss as possible) and see if that gives us a clue about what makes this problem tough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale the datasets\n",
    "scale_transformer = StandardScaler(copy=True).fit(x_train)\n",
    "x_train_scaled = scale_transformer.transform(x_train)\n",
    "x_test_scaled = scale_transformer.transform(x_test)\n",
    "\n",
    "\n",
    "# reduce dimensions\n",
    "pca_transformer = PCA(2).fit(x_train_scaled)\n",
    "x_train_2d = pca_transformer.transform(x_train_scaled)\n",
    "x_test_2d =  pca_transformer.transform(x_test_scaled)\n",
    "\n",
    "x_train_2d[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments:\n",
    "1. Both scaling and reducing dimension follow the same pattern: we fit the object to the training data, then use .transform to convert the training and test data. This ensures that, for instance, we scale the test data using the _training_ mean and variance, not its own mean and variance\n",
    "2. We need to equalize the variance of each feature before applying PCA, otherwise certain dimensions will dominate the scaling: our PCA dimensions would just be the features with the largest spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot each group\n",
    "\n",
    "# notice that we set up lists to track each group's plotting color and label\n",
    "colors = ['r','c','b']\n",
    "label_text = [\"Bad Wines\", \"Medium Wines\", \"Good Wines\"]\n",
    "\n",
    "# and we loop over the different groups\n",
    "for cur_quality in [0,1,2]:\n",
    "    cur_df = x_train_2d[y_train==cur_quality]\n",
    "    plt.scatter(cur_df[:,0], cur_df[:,1], c = colors[cur_quality], label=label_text[cur_quality])\n",
    "    \n",
    "# all plots need labels\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimention 2\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that gives us some idea of why the problem is difficult: the good wines and bad wines are hiding right among the average wines. It does look like the wines separate into two groups, though, possibly one for reds and one for whites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"><b>Discussion</b></div>\n",
    "1. What critique can you make against the plot above? Why does this plot not prove that the different wines are hopelessly similar?\n",
    "2. Which of LDA/QDA would be appropriate to this 2d data?\n",
    "3. The wine data we've used so far consist entirely of continuous predictors. Would PCA work with categorical data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 6</b></div>\n",
    "1. Edit the code above to plot the locations of red wines and white wines\n",
    "2. Fit an LDA or QDA (whichever is appropriate) to the 2d data, classifying the red and white wines. Remember to compare to a trivial model.\n",
    "3. Something is technically wrong with the fit above... If you have time, identify and correct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating PCA -  Variance Explained\n",
    "One of the criticisms we made of the PCA plot was that it's lost something from the original data.\n",
    "\n",
    "Let's actually investigate how much of the original data's structure the 2d PCA captures. We'll look at the `explained_variance_ratio_` portion of the PCA fit. This lists, in order, the percentage of the x data's total variance that is captured by the nth PCA dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_explained = pca_transformer.explained_variance_ratio_\n",
    "print(\"Variance explained by each PCA component:\", var_explained)\n",
    "print(\"Total Variance Explained:\", np.sum(var_explained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first PCA dimension captures 33% of the variance in the data, and the second PCA dimension adds another 20%. Together, we've got about half of the total variation in the training data covered with just these two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 7</b></div>\n",
    "1. Fit a PCA that finds the first 10 PCA components of our training data\n",
    "2. Use `np.cumsum` to print out the variance we'd be able to explain by using n PCA dimensions for n=1 through 10\n",
    "3. Does the 10-dimension PCA agree with the 2d PCA on how much variance the first components explain? Do the 10d and 2d PCAs find the same first two dimensions? Why or why not?\n",
    "4. Make a plot of number of PCA dimensions against total variance explained. What PCA dimension looks good to you?\n",
    "\n",
    "Hint: `np.cumsum` stands for 'cumulative sum', so `np.cumsum([1,3,2,-1,2])` is `[1,4,6,5,7]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- PCA maps a high-dimensional space into a lower dimensional space.\n",
    "- The PCA dimensions are ordered by how much of the original data's variance they capture\n",
    "    - There are other cool and useful properties of the PCA dimensions (orthogonal, etc.). See a [textbook](http://math.mit.edu/~gs/linearalgebra/).\n",
    "- PCA on a given dataset always gives the same dimensions in the same order.\n",
    "- You can select the number of dimensions by fitting a big PCA and examining a plot of the cumulative variance explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Did we fail?\n",
    "None of the models worked, and we can't tell good wines from bad. Was it all a waste of time and money?\n",
    "\n",
    "Not really. All analyses are a roll of the dice. Some analyses fail, like this one did, becuase the data at hand just don't support the task we've set out.\n",
    "\n",
    "What can we do about it?\n",
    "1. Be honest about the methods and the null result. Lots of analyses fail.\n",
    "2. Collect a dataset you think has a better chance of success. Maybe we collected the wrong chemical signals...\n",
    "3. Keep trying new approaches. Just beware of overfitting the data you're validating on. Always have a test set locked away for when the final model is built.\n",
    "4. Change the question. Maybe something you noticed during analysis seems interesting or useful (classifying red versus white). But again, you the more you try, the more you might overfit, so have test data locked away.\n",
    "5. Just move on. If the odds of success start to seem small, maybe you need a new project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Moral of the Lab\n",
    "- Sometimes, the data just aren't enough to adequately predict outcomes.\n",
    "- In this lab we saw that no amount of modeling finesse would let us use a wine's chemical properties to tell good wines and bad wines from mediocre ones.\n",
    "    - The chemical properties were very good at telling red wines from whites, however.\n",
    "- PCA helped us visualize the data and confirm that the highly rated wines just aren't chemically distinct from the other wines.\n",
    "- **NOT ALL ANALYSES YIELD USEFUL RESULTS** Sometimes (arguably most of the time), the data aren't suitable for a task or just don't have anything interesting to say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 (Sidebar): Pipelines\n",
    "Remember when we were trying to adapt our LDA model to run on the training data with 'red' dropped? We had to invent new variable names and define functions and it was generally much harder than it needed to be. Pipelines are `sklearn`'s tool for packaging an entire analysis together into a single object. This enables convenient inspection, saving, deployment, and (yes) cross validation of the model.\n",
    "\n",
    "Let's look at an example (we'll switch the model to KNN to justify some later analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "knn_pipeline = Pipeline(\n",
    "    [\n",
    "        ('scaling', StandardScaler()), # scale all columns\n",
    "        ('dim_reduction', PCA()), # PCA to reduce dimension\n",
    "        ('model', KNeighborsClassifier()) # KNN to predict\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# run with default settings ()\n",
    "knn_pipeline.fit(x_train, y_train)\n",
    "print(\"Test set score (default parameters)\", knn_pipeline.score(x_test, y_test))\n",
    "\n",
    "# particular sub-component settings are accessed with the component name, two\n",
    "# underscores, and the parameter name\n",
    "knn_pipeline.set_params(dim_reduction__n_components = 2, model__n_neighbors = 5)\n",
    "knn_pipeline.fit(x_train, y_train)\n",
    "print(\"Test set score (updated parameters)\", knn_pipeline.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a convenience function `make_pipeline` that lets us skip naming the different steps. Notice the default names are all-lowercase versions of the class names (standardscaler, pca, kneighborsclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "knn_pipeline = make_pipeline(StandardScaler(), PCA(), KNeighborsClassifier())\n",
    "knn_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to run the whole modelling process on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_model = knn_pipeline.fit(x_train.drop('red', axis=1), x_train['red'])\n",
    "red_model.score(x_test.drop('red', axis=1), x_test['red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, cross validation tools work directly with the pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(knn_pipeline, x_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "search_dict = {\n",
    "    'pca__n_components': [3,5,10],\n",
    "    'kneighborsclassifier__n_neighbors': [1,2,3,4,5]\n",
    "}\n",
    "cv_results = GridSearchCV(knn_pipeline, search_dict, cv=3).fit(x_train, y_train)\n",
    "cv_results.best_params_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In general, more PCA components will work better for prediction. However, KNN often performs worse as dimension increases, meaning there may be a meaningful balance point between capturing more variance and a space small enough for KNN to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8 (Sidebar): Weighting the training points\n",
    "Some models can accept weights on the training points to given them greater priority in the model's fitting process. This can be useful if, for instance, certain classes are rare but we want to be sure the model classifies them correctly (e.g. we're trying to classify cancers and one form is rare but very aggressive). In general, weighting training points is like moving along the ROC curve; we change some model parameters to alter the mistakes the model makes to be more in line with our tastes.\n",
    "\n",
    "Let's see this in action with a logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_lr = LogisticRegression(C=1000000).fit(x_train,y_train)\n",
    "\n",
    "weight_dict = {0:100, 1:1, 2:100}\n",
    "weighted_lr = LogisticRegression(C=1000000, class_weight=weight_dict).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Rows: True Lables (Bad, Medium, Good), \\nColummns: Predicted Lables (Bad, Medium, Good)\")\n",
    "print()\n",
    "print(\"unweighted:\")\n",
    "print(confusion_matrix(y_test, unweighted_lr.predict(x_test)))\n",
    "print(\"weighted:\")\n",
    "print(confusion_matrix(y_test, weighted_lr.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without weighting, the model plays it safe and predicts that all of the test set wines are medium. With weighting, the model is told to care more about getting the bad and good wines right. The model does as we've asked and correctly IDs 3 good/bad test wines, at the price of 17 falsely bad wines and 16 falsely good wines. However, if identifying bad and good wines is, as implied, 100 times more important than identifying medium wines, we've made a really good trade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 8</b></div>\n",
    "1. What happens if you give a weight of 0 to the medium wines?\n",
    "2. What weighting gives results that accord with your personal sense of what the model should be doing? How many actually-medium bottles is a single good bottle worth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

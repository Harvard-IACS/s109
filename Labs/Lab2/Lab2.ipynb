{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"iacs.png\"> S-109A Introduction to Data Science \n",
    "\n",
    "\n",
    "## Lab 2: `numpy`,plotting, KNN Regression, Simple Linear Regression\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2018**<br>\n",
    "**Instructors:** Pavlos Protopapas and Kevin Rader<br>\n",
    "**Lab Instructors:** David Sondak and Will Claybaugh\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Run the cell below to properly highlight the exercises</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.exercise { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"<style>div.exercise { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "<ol start=\"0\">\n",
    "  <li> Learning Goals </li>\n",
    "  <li> `numpy` </li>\n",
    "  <li> Creating plots </li>\n",
    "  <li> Simple linear regression  </li>\n",
    "  <li> $k$-nearest neighbors</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "Overall description and goal for the lab.\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Use `numpy` proficiently and efficiently\n",
    "* Make great, readable, informative plots with `matplotlib`\n",
    "* Feel comfortable with simple linear regression\n",
    "* Feel comfortable with $k$ nearest neighbors\n",
    "\n",
    "**This lab corresponds to lecture 3 and maps on to homework 2 (and beyond).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> DO PART 1 BEFORE COMING TO LAB</font>\n",
    "\n",
    "## Part 1: Numerical Python:  `numpy`\n",
    "Scientific `Python` code uses a fast array structure, called the `numpy` array. Those who have worked in `Matlab` will find this very natural.   For reference, the `numpy` documention can be found here: [`numpy`](http://www.numpy.org/).  \n",
    "\n",
    "\n",
    "Let's make a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.array([1,4,9,16])\n",
    "my_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays support the same operations as lists! Below we compute length, slice, and iterate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(array):\", len(my_array)) # Length of array\n",
    "\n",
    "print(\"array[2:4]:\", my_array[2:4]) # A slice of the array\n",
    "\n",
    "# Iterate over the array\n",
    "for ele in my_array:\n",
    "    print(\"element:\", ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In general you should manipulate numpy arrays by using numpy module functions** (e.g. `np.mean`). This is for efficiency purposes, and a discussion follows below this section.\n",
    "\n",
    "You can calculate the mean of the array elements either by calling the method `.mean` on a numpy array or by applying the function `np.mean` with the `numpy` array as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two ways of calculating the mean\n",
    "\n",
    "print(my_array.mean())\n",
    "\n",
    "print(np.mean(my_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we constructed the `numpy` array above seems redundant. After all we already had a regular `python` list. Indeed, it is the other ways we have to construct `numpy` arrays that make them super useful. \n",
    "\n",
    "There are many such `numpy` array *constructors*. Here are some commonly used constructors. Look them up in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros(10) # generates 10 floating point zeros\n",
    "zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Numpy` gains a lot of its efficiency from being strongly typed. That is, all elements in the array have the same type, such as integer or floating point. The default type, as can be seen above, is a float of size appropriate for the machine (64 bit on a 64 bit machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones(10, dtype='int') # generates 10 integer ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you will want random numbers. Use the `random` constructor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(10) # uniform on [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generate random numbers from a normal distribution with mean $0$ and variance $1$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_array = np.random.randn(1000000)\n",
    "print(\"The sample mean and standard devation are {0:17.16f} and {1:17.16f}, respectively.\".format(np.mean(normal_array), np.std(normal_array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `numpy` supports vector operations\n",
    "\n",
    "What does this mean? It means that to add two arrays instead of looping ovr each element (e.g. via a list comprehension as in base Python) you get to simply put a plus sign between the two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_array = np.ones(5)\n",
    "twos_array = 2*np.ones(5)\n",
    "ones_array + twos_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this behavior is very different from `python` lists, which just get longer when you try to + them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_list = [1., 1., 1., 1., 1.]\n",
    "second_list = [1., 1., 1., 1., 1.]\n",
    "first_list + second_list # not what you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On some computer chips nunpy's addition actually happens in parallel, so speedups can be high. But even on regular chips, the advantage of greater readability is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Numpy` supports a concept known as *broadcasting*, which dictates how arrays of different sizes are combined together. There are too many rules to list all of them here.  Here are two important rules:\n",
    "\n",
    "1. Multiplying an array by a number multiplies each element by the number\n",
    "2. Adding a number adds the number to each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_array + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 * ones_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that if you wanted the distribution $N(5, 7)$ you could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_5_7 = 5.0 + 7.0 * normal_array\n",
    "\n",
    "np.mean(normal_5_7), np.std(normal_5_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen how to create and work with simple one dimensional arrays in `numpy`.  You have also been introduced to some important `numpy` functionality (e.g. `mean` and `std`).\n",
    "\n",
    "Next, we push ahead to two-dimensional arrays and begin to dive into some of the deeper aspects of `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D arrays\n",
    "We can create two-dimensional arrays without too much fuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a 2d-array by handing a list of lists\n",
    "my_array2d = np.array([ \n",
    "    [1, 2, 3, 4], \n",
    "    [5, 6, 7, 8], \n",
    "    [9, 10, 11, 12] \n",
    "])\n",
    "\n",
    "# you can do the same without the pretty formatting (decide which style you like better)\n",
    "my_array2d = np.array([ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12] ])\n",
    "\n",
    "\n",
    "# 3 x 4 array of ones\n",
    "ones_2d = np.ones([3, 4])\n",
    "print(ones_2d, \"\\n\")\n",
    "\n",
    "# 3 x 4 array of ones with random noise\n",
    "ones_noise = ones_2d + 0.01*np.random.randn(3, 4)\n",
    "print(ones_noise, \"\\n\")\n",
    "\n",
    "# 3 x 3 identity matrix\n",
    "my_identity = np.eye(3)\n",
    "print(my_identity, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like lists, `numpy` arrays are $0$-indexed.  Thus we can access the $n$th row and the $m$th column of a two-dimensional array with the indices $[n - 1, m - 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_array2d)\n",
    "print(\"element [2,3] is:\", my_array2d[2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays can be sliced, and can be iterated over with loops.  Below is a schematic illustrating slicing two-dimensional arrays.  \n",
    "\n",
    " <img src=\"images/2dindex_v2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    " \n",
    "Notice that the list slicing syntax still works!  \n",
    "`array[2:,3]` says \"in the array, get rows 2 through the end, column 3]\"  \n",
    "`array[3,:]` says \"in the array, get row 3, all columns\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy functions will by default work on the entire array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ones_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The axis `0` is the one going downwards (i.e. the rows), whereas axis `1` is the one going across (the columns). You will often use functions such as `mean` or `sum` along a particular axis. If you `sum` along axis 0 you are summing across the rows and will end up with one value per column. As a rule, any axis you list in the axis argument will dissapear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ones_2d, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ones_2d, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "* Create a two-dimensional array of size $3\\times 5$ and do the following:\n",
    "  * Print out the array\n",
    "  * Print out the shape of the array\n",
    "  * Create two slices of the array:\n",
    "    1. The first slice should be the last row and the third through last column\n",
    "    2. The second slice should be rows $1-3$ and columns $3-5$\n",
    "  * Square each element in the array and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `numpy` supports matrix operations\n",
    "2d arrays are numpy's way of representing matrices. As such there are lots of built-in methods for manipulating them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier when we generated the one-dimensional arrays of ones and random numbers, we gave `ones` and `random`  the number of elements we wanted in the arrays. In two dimensions, we need to provide the shape of the array, i.e., the number of rows and columns of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "three_by_four = np.ones([3,4])\n",
    "three_by_four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can transpose the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_by_four.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_by_three = three_by_four.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_by_three.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication is accomplished by `np.dot`. The `*` operator will do element-wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.dot(three_by_four, four_by_three)) # 3 x 3 matrix\n",
    "np.dot(four_by_three, three_by_four) # 4 x 4 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has functions to do the difficult matrix operations that are awful to do by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.random.rand(4,4) # a 4 by 4 matrix\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the eigenvalues and eigenvectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eig(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about inverses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_matrix = np.linalg.inv(matrix) # the invert matrix\n",
    "print(inv_matrix)\n",
    "\n",
    "#prove it's the inverse\n",
    "np.dot(matrix,inv_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is a bit of 'rounding error' in the inverse calculation. This is becuase the computer can't store the exact values the inverse matrix asks for (they have more decimal places than the computer can hold). Built-in numpy routines manage these errors, which is why it's vrey important to use pre-built tools whenever possible, and to be very cautious when writing your own.\n",
    "\n",
    "(It happens that there are even more advanced numpy functions like `np.linalg.solve` which are more accurate than just taking the naked inverse)\n",
    "\n",
    "See the documentation to learn more about `numpy` functions as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy's layout in the computer's memory\n",
    "**Advanced coders**: You should notice that access is row-by-row and one dimensional iteration gives a row. This is because `numpy` lays out memory row-wise. <br><br>\n",
    "**Starting coders**: If you really need a particular section of numpy to run faster, ask an advanced coder to point out improvements in your code. They may have you transpose your arrays. But if your code is fast enough, then it's fast enough.\n",
    "\n",
    " <img src=\"https://aaronbloomfield.github.io/pdr/slides/images/04-arrays-bigoh/2d-array-layout.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    " \n",
    "(from https://aaronbloomfield.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An often seen idiom allocates a two-dimensional array, and then fills in one-dimensional arrays from some function.  We will discuss why this is a bad design pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate a 2D array\n",
    "twod = np.zeros([5, 2])\n",
    "print(twod, \"\\n\")\n",
    "\n",
    "# Fill in with random numbers\n",
    "for i in range(twod.shape[0]):\n",
    "    twod[i, :] = np.random.random(2)\n",
    "print(twod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this and many other cases, it is faster to simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twod = np.random.random(size=(5,2))\n",
    "twod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Numpy `Arrays vs. `Python` Lists?\n",
    "\n",
    "1. Why the need for `numpy` arrays?  Can't we just use `Python` lists?\n",
    "2. Iterating over `numpy` arrays is slow. Slicing is faster.\n",
    "\n",
    "`Python` lists may contain items of different types. This flexibility comes at a price: `Python` lists store *pointers* to memory locations.  On the other hand, `numpy` arrays are typed, where the default type is floating point.  Because of this, the system knows how much memory to allocate, and if you ask for an array of size $100$, it will allocate one hundred contiguous spots in memory, where the size of each spot is based on the type.  This makes access extremely fast.\n",
    "\n",
    "<img src=\"https://jakevdp.github.io/PythonDataScienceHandbook/figures/array_vs_list.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "(from the book below)\n",
    "\n",
    "Unfortunately, looping over an array slows things down. In general you should not access `numpy` array elements by iteration.  This is because of type conversion.  `Numpy` stores integers and floating points in `C`-language format.  When you operate on array elements through iteration, `Python` needs to convert that element to a `Python` `int` or `float`, which is a more complex beast (a `struct` in `C` jargon).  This has a cost.\n",
    "\n",
    "<img src=\"https://jakevdp.github.io/PythonDataScienceHandbook/figures/cint_vs_pyint.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "(from the book below)\n",
    "\n",
    "If you want to know more, we will suggest that you read [Jake Vanderplas's Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). You will find that book an incredible resource for this class.\n",
    "\n",
    "Why is slicing faster? The reason is technical: slicing provides a *view* onto the memory occupied by a `numpy` array, instead of creating a new array. That is the reason the code above this cell works nicely as well. However, if you iterate over a slice, then you have gone back to the slow access.\n",
    "\n",
    "By contrast, functions such as `np.dot` are implemented at `C`-level, do not do this type conversion, and access contiguous memory. If you want this kind of access in `Python`, use the `struct` module or `Cython`. Indeed many fast algorithms in `numpy`, `pandas`, and `C` are either implemented at the `C`-level, or employ `Cython`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Check:\n",
    "Answer these questions and see the bottom of the lab to check your answers.\n",
    "\n",
    "1. What is the major benefit of working in `numpy`?\n",
    "2. Why is it vital to use built-in numpy functions whenever possible?\n",
    "3. You need to find the running total of each element in a numpy array. For example, the input \\[2,5,3,2\\] should give the output \\[2,7,10,12\\]. Rather than writing your own code, think of at least two google querrys to find a relevant numpy function.\n",
    "4. What `numpy` function does the job above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> You're done. Save the rest for lab on Friday</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Plotting\n",
    "Conveying your findings convincingly is an absolutely crucial part of any analysis. Therefore, you must be able to write well and make compelling visuals.  Creating informative visuals is an involved process and we won't cover that in this lab.  However, part of creating informative data visualizations means generating *readable* figures.  If people can't read your figures or have a difficult time interpreting them, they won't understand the results of your work.  Here are some non-negotiable commandments for any plot:\n",
    "* Label $x$ and $y$ axes\n",
    "* Axes labels should be informative\n",
    "* Axes labels should be large enough to read\n",
    "* Make tick labels large enough\n",
    "* Include a legend if necessary\n",
    "* Include a title if necessary\n",
    "* Use appropriate line widths\n",
    "* Use different line styles for different lines on the plot\n",
    "* Use different markers for different lines\n",
    "\n",
    "There are other important elements, but that list should get you started on your way.\n",
    "\n",
    "Here is the anatomy of a figure:\n",
    " <img src=\"https://tacaswell.github.io/matplotlib/_images/anatomy.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    " \n",
    "taken from [showcase example code: anatomy.py](https://tacaswell.github.io/matplotlib/examples/showcase/anatomy.html).\n",
    "\n",
    "We will work with `matplotlib` and `seaborn` for plotting in this class.  Today's lab will focus on `matplotlib`, which is a very powerful `python` library for making scientific plots.  `seaborn` is a little more specialized in that it was developed for statistical data visualization.  We will put in a little bit of `seaborn` at the end, but if you are comfortable with `matplotlib` then `seaborn` will be a breeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving in, one more note should be made.  We will not focus on the internal aspects of `matplotlib`.  Today's lab will really only focus on the basics and developing good plotting practices.  There are many excellent tutorials out there for `matplotlib`.  For example,\n",
    "* [`matplotlib` homepage](https://matplotlib.org/)\n",
    "* [`matplotlib` tutorial](https://github.com/matplotlib/AnatomyOfMatplotlib)\n",
    "\n",
    "Okay, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "Later on in this lab, we will use the following three functions to make some plots:\n",
    "\n",
    "* Logistic function:\n",
    "  \\begin{align*}\n",
    "    f\\left(z\\right) = \\dfrac{1}{1 + be^{-az}}\n",
    "  \\end{align*}\n",
    "  where $a$ and $b$ are parameters.\n",
    "* Hyperbolic tangent:\n",
    "  \\begin{align*}\n",
    "    g\\left(z\\right) = b\\tanh\\left(az\\right) + c\n",
    "  \\end{align*}\n",
    "  where $a$, $b$, and $c$ are parameters.\n",
    "* Rectified Linear Unit:\n",
    "  \\begin{align*}\n",
    "    h\\left(z\\right) = \n",
    "    \\left\\{\n",
    "      \\begin{array}{lr}\n",
    "        z, \\quad z > 0 \\\\\n",
    "        \\epsilon z, \\quad z\\leq 0\n",
    "      \\end{array}\n",
    "    \\right.\n",
    "  \\end{align*}\n",
    "  where $\\epsilon < 0$ is a small, positive parameter.\n",
    "\n",
    "You are given the code for the first two functions.  Notice that $z$ is passed in as a `numpy` array and that the functions are returned as `numpy` arrays.  Parameters are passed in as floats.\n",
    "\n",
    "You should write a function to compute the rectified linear unit.  The input should be a `numpy` array for $z$ and a positive float for $\\epsilon$.  The next cell provides functions to compute the logistic and hyperbolic tangent functions.  You might want to use these as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic(z: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    \"\"\" Compute logistic function\n",
    "      Inputs:\n",
    "         a: exponential parameter\n",
    "         b: exponential prefactor\n",
    "         z: numpy array; domain\n",
    "      Outputs:\n",
    "         f: numpy array of floats, logistic function\n",
    "    \"\"\"\n",
    "    \n",
    "    den = 1.0 + b * np.exp(-a * z)\n",
    "    return 1.0 / den\n",
    "\n",
    "def stretch_tanh(z: np.ndarray, a: float, b: float, c: float) -> np.ndarray:\n",
    "    \"\"\" Compute stretched hyperbolic tangent\n",
    "      Inputs:\n",
    "         a: horizontal stretch parameter (a>1 implies a horizontal squish)\n",
    "         b: vertical stretch parameter\n",
    "         c: vertical shift parameter\n",
    "         z: numpy array; domain\n",
    "      Outputs:\n",
    "         g: numpy array of floats, stretched tanh\n",
    "    \"\"\"\n",
    "    return b * np.tanh(a * z) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some plots.  First, let's just warm up and plot the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.0, 5.0, 100) # Equally spaced grid of 100 pts between -5 and 5\n",
    "\n",
    "f = logistic(x, -1.0, 1.0) # Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is only needed in Jupyter notebooks!  Displays the plots for us.\n",
    "%matplotlib inline \n",
    "\n",
    "plt.plot(x, f); # Use the semicolon to suppress some iPython output (not needed in real Python scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful!  We have a plot.  It's terribly ugly and uninformative.  Let's clean it up a bit by putting some labels on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "plt.title('Logistic Function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it's getting better.  Still super ugly.  I see these kinds of plots at conferences all the time.  Unreadable.  We can do better.  Much, much better.  First, let's throw on a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "plt.title('Logistic Function')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our plot is starting to get a little better but also a little crowded.\n",
    "\n",
    "#### A note on gridlines\n",
    "Gridlines can be very helpful in many scientific disciplines.  They help the reader quickly pick out important points and limiting values.  On the other hand, they can really clutter the plot.  Some people recommend never using gridlines, while others insist on them being present.  The correct approach is probably somewhere in between.  Use gridlines when necessary, but dispense with them when they take away more than they provide.  Ask yourself if they help bring out some important conclusion from the plot.  If not, then best just keep them away.\n",
    "\n",
    "Before proceeding any further, I'm going to change notation.  The plotting interface we've been working with so far is okay, but not as flexible as it can be.  In fact, I don't usually generate my plots with this interface.  I work with slightly lower-level methods, which I will introduce to you now.  The reason I need to make a big deal about this is because the lower-level methods have a slightly different API.  This will become apparent in my next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1) # Get figure and axes objects\n",
    "\n",
    "ax.plot(x, f) # Make a plot\n",
    "\n",
    "# Create some labels\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f')\n",
    "ax.set_title('Logistic Function')\n",
    "\n",
    "# Grid\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, it's *exactly* the same plot!  Notice, however, the use of `ax.set_xlabel()` instead of `plt.xlabel()`.  The difference is tiny, but you should be aware of it.  I will use this plotting syntax from now on.\n",
    "\n",
    "What else do we need to do to make this figure better?  Here are some options:\n",
    "* Make labels bigger!\n",
    "* Make line fatter\n",
    "* Make tick mark labels bigger\n",
    "* Make the grid less pronounced\n",
    "* Make figure bigger\n",
    "\n",
    "Let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6)) # Make figure bigger\n",
    "\n",
    "ax.plot(x, f, lw=4) # Linewidth bigger\n",
    "ax.set_xlabel('x', fontsize=24) # Fontsize bigger\n",
    "ax.set_ylabel('f', fontsize=24) # Fontsize bigger\n",
    "ax.set_title('Logistic Function', fontsize=24) # Fontsize bigger\n",
    "ax.grid(True, lw=1.5, ls='--', alpha=0.75) # Update grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice:\n",
    "* `lw` stands for `linewidth`.  We could also write `ax.plot(x, f, linewidth=4)`\n",
    "* `ls` stands for `linestyle`.\n",
    "* `alpha` stands for transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are looking good now!  Unfortunately, people still can't read the tick mark labels.  Let's remedy that presently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6)) # Make figure bigger\n",
    "\n",
    "# Make line plot\n",
    "ax.plot(x, f, lw=4)\n",
    "\n",
    "# Update ticklabel size\n",
    "ax.tick_params(labelsize=24)\n",
    "\n",
    "# Make labels\n",
    "ax.set_xlabel(r'$x$', fontsize=24) # Use TeX for mathematical rendering\n",
    "ax.set_ylabel(r'$f(x)$', fontsize=24) # Use TeX for mathematical rendering\n",
    "ax.set_title('Logistic Function', fontsize=24)\n",
    "\n",
    "ax.grid(True, lw=1.5, ls='--', alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing remaining to do is to change the $x$ limits.  Clearly these should go from $-5$ to $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6)) # Make figure bigger\n",
    "\n",
    "# Make line plot\n",
    "ax.plot(x, f, lw=4)\n",
    "\n",
    "# Set axes limits\n",
    "ax.set_xlim(x.min(), x.max())\n",
    "\n",
    "# Update ticklabel size\n",
    "ax.tick_params(labelsize=24)\n",
    "\n",
    "# Make labels\n",
    "ax.set_xlabel(r'$x$', fontsize=24) # Use TeX for mathematical rendering\n",
    "ax.set_ylabel(r'$f(x)$', fontsize=24) # Use TeX for mathematical rendering\n",
    "ax.set_title('Logistic Function', fontsize=24)\n",
    "\n",
    "ax.grid(True, lw=1.5, ls='--', alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with figures forever making them perfect.  At this point, everyone can read and interpret this figure just fine.  Don't spend your life making the perfect figure.  Make it good enough so that you can convey your point to your audience.  Then save if it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('logistic.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!  Let's take a look.\n",
    "![](logistic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "If you want to see all the styles available, please take a look at the documentation.\n",
    "* [Line styles](https://matplotlib.org/2.0.1/api/lines_api.html#matplotlib.lines.Line2D.set_linestyle)\n",
    "* [Marker styles](https://matplotlib.org/2.0.1/api/markers_api.html#module-matplotlib.markers)\n",
    "* [Everything you could ever want](https://matplotlib.org/2.0.1/api/lines_api.html#matplotlib.lines.Line2D.set_marker)\n",
    "\n",
    "We haven't discussed it yet, but you can also put a legend on a figure.  You'll do that in the next exercise.  Here are some additional resources:\n",
    "* [Legend](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html)\n",
    "* [Grid](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.grid.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "Do the following:\n",
    "* Make a figure with the logistic function, hyperbolic tangent, and rectified linear unit.\n",
    "* Use different line styles for each plot\n",
    "* Put a legend on your figure\n",
    "* Save your figure\n",
    "\n",
    "Here's an example of a figure:\n",
    "![](nice_plots.png)\n",
    "\n",
    "You don't need to make the exact same figure, but it should be just as nice and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a many more things you can do to the figure to spice it up.  Remember, there must be a tradeoff between making a figure look good and the time you put into it.  \n",
    "\n",
    "**The guiding principle should be that your audience needs to easily read and understand your figure.**\n",
    "\n",
    "There are of course other types of figures including, but not limited to, \n",
    "* Scatter plots (you will use these all the time)\n",
    "* Bar charts\n",
    "* Histograms\n",
    "* Contour plots\n",
    "* Surface plots\n",
    "* Heatmaps\n",
    "\n",
    "There is documentation on each one of these.  You should feel comforatable enough with the plotting API now to dig in and make readable, understandable plots.\n",
    "\n",
    "Before moving on, we will discuss another way to make your plots look good without all the hassle.  I'll make a beautiful plot without having to specify annoying arguments every single time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config # User-defined config file\n",
    "plt.rcParams.update(config.pars) # Update rcParams to make nice plots\n",
    "\n",
    "# First get the data\n",
    "f1 = logistic(x, -1.0, 1.0)\n",
    "f2 = logistic(x, -2.0, 1.0)\n",
    "f3 = logistic(x, -3.0, 1.0)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6)) # Create figure object\n",
    "\n",
    "# Make actual plots\n",
    "# (Notice the label argument!)\n",
    "ax.plot(x, f1, ls='-', label=r'$L(x;-1)$')\n",
    "ax.plot(x, f2, ls='--', label=r'$L(x;-2)$')\n",
    "ax.plot(x, f3, ls='-.', label=r'$L(x;-3)$')\n",
    "\n",
    "# Set axes limits to make the scale nice\n",
    "ax.set_xlim(x.min(), x.max())\n",
    "ax.set_ylim(h.min(), 1.1)\n",
    "\n",
    "# Make readable labels\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$h(x)$')\n",
    "ax.set_title('Logistic Functions')\n",
    "\n",
    "# Set up grid\n",
    "ax.grid(True, lw=1.75, ls='--', alpha=0.75)\n",
    "\n",
    "# Put legend on figure\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a good-looking plot!  Notice that we didn't need to have all those annoying `fontsize` specifications floating around.  If you want to reset the defaults, just use `plt.rcdefaults()`.\n",
    "\n",
    "Now, how in the world did this work?  Obviously, there is something special about the `config` file.  I didn't give you a config file, but the next exercise requires you to create one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "* Read the *matplotlib rcParams* section at the following page: [Customizing matplotlib](https://matplotlib.org/users/customizing.html)\n",
    "* Create your very own `config.py` file.  It should have the following structure:\n",
    "```python\n",
    "pars = {}\n",
    "```\n",
    "You must fill in the `pars` dictionary yourself.  All the possible parameters can be found at the link above.  For example, if you want to set a default line width of `4`, then you would have \n",
    "```python\n",
    "pars = {'lines.linewidth': 4}\n",
    "```\n",
    "  in your `config.py` file.\n",
    "* Make sure your `config.py` file is in the same directory as your lab notebook.\n",
    "* Make a plot (similar to the one I made above) using your `config` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `seaborn`\n",
    "Early on in this plotting section, I mentioned `seaborn`.  You can use `seaborn` to make very nice plots of statistical data.  Here is the main website:  [seaborn: statistical data visualization](https://seaborn.pydata.org/).\n",
    "\n",
    "We won't dive deep into `seaborn` here.  It is quite popular in the data science community, but it is ultimately up to you whether or not you choose to use it.\n",
    "\n",
    "`seaborn` works great with `pandas`.  It can also be customized easily.  Here is the basic `seaborn` tutorial: [Seaborn tutorial](https://seaborn.pydata.org/tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Excuses\n",
    "With all of these resourses, there is no reason to have a bad figure. EVER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll move on to some data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Simple Linear Regression\n",
    "Linear regression and its many extensions are a workhorse of the statistics and data science community, both in application and as a reference point for other models. Most of the major concepts in machine learning can be and often are discussed in terms of various linear regression models. Thus, this section will introduce you to building and fitting linear regression models and some of the process behind it, so that you can 1) fit models to data you encounter 2) experiment with different kinds of linear regression and observe their effects 3) see some of the technology that makes regression models work.\n",
    "\n",
    "\n",
    "### Part 3.1: Linear regression with a toy dataset\n",
    "We first examine a toy problem, focusing our efforts on fitting a linear model to a small dataset with three observations.  Each observation consists of one predictor $x_i$ and one response $y_i$ for $i = 1, 2, 3$,\n",
    "\n",
    "\\begin{align*}\n",
    "(x , y) = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}.\n",
    "\\end{align*}\n",
    "\n",
    "To be very concrete, let's set the values of the predictors and responses.\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(1, 2), (2, 2), (3, 4)\\}\n",
    "\\end{equation*}\n",
    "\n",
    "There is no line of the form $\\beta_0 + \\beta_1 x = y$ that passes through all three observations, since the data are not collinear. Thus our aim is to find the line that best fits these observations in the *least-squares sense*, as discussed in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [2,2,4]\n",
    "plt.scatter(x_data,y_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Formulae\n",
    "Linear regression is special among the models we study beuase it can be solved explicitly. While most other models (and even some advanced versions of linear regression) must be solved itteratively, linear regression has a formula where you can simply plug in the data.\n",
    "\n",
    "For the single predictor case it is:\n",
    "    \\begin{align}\n",
    "      \\beta_1 &= \\frac{\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sum_{i=1}^n{(x_i-\\bar{x})^2}}\\\\\n",
    "      \\beta_0 &= \\bar{y} - \\beta_1\\bar{x}\\\n",
    "    \\end{align}\n",
    "    \n",
    "Where $\\bar{y}$ and $\\bar{x}$ are the mean of the y values and the mean of the x values, respectively.\n",
    "\n",
    "\n",
    "In words, \n",
    "    \\begin{align}\n",
    "      \\text{optimal slope} &= \\frac{\\sum_{\\text{all observations}}{(\\text{x's deviation from mean})(\\text{y's deviation from mean})}}{\\sum_{\\text{all observations}}{\\text{x's squared deviation from mean}}}\\\\\n",
    "      \\text{opimal intercept} &= \\text{mean y} - \\text{optimal slope}\\cdot\\text{mean x}\\\n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "In statistics:\n",
    "    \\begin{align}\n",
    "      \\beta_1 &= \\frac{Cov(x,y)}{Var(x)}\\\\\n",
    "      \\bar{y} &= \\beta_0 + \\beta_1\\bar{x}\\\n",
    "    \\end{align}\n",
    "\n",
    "From the re-aranged second equation we can see that the best-fit line  passes through $(\\bar{x},\\bar{y})$, the center of mass of the data\n",
    "\n",
    "From any of the first equations, we can see that the slope of the line has to do with whether or not an x value that is above/below the center of mass is typically paired with a y value that is likewise above/below, or typically paired with one that is opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices and Math\n",
    "Okay, but where did these equations come from?\n",
    "\n",
    "We won't do a full derivation here, but the logic goes like this:\n",
    "1. We want to pick $\\beta_0$ and $\\beta_1$ so as to minimize the squared error. So, we need to write the squared error in terms of $\\beta_0$ and $\\beta_1$. It happens to be $$ error = \\sum(true - predicted)^2 = \\sum_{i=1}^n\\left(y_i - (\\beta_0 + \\beta_1x_i)^2\\right)$$\n",
    "2. This is a formula we want to minimize, so we apply calculus. The derivative with respect to $beta_1$ needs to be 0, as does the derivative with respect to $beta_0$.\n",
    "3. Taking the derivatives and solving, we find the formulae above.\n",
    "\n",
    "If we follow the above process using matrices instead of vectors, (after a ton of ugly algebra) we arrive at a final set of equations (the ones you'll implement on your homework)\n",
    "\n",
    "\\begin{align}\n",
    "  \\beta = (X^T X)^{-1} X^T Y.\n",
    "\\end{align}\n",
    "\n",
    "Here, $X$ is a full dataset (one row per observation, one column per predictor), and $Y$ is still a column vector holding the response variable values. $\\beta$ is a vector of coeficients, one per feature.\n",
    "\n",
    "#### Working with data matrices\n",
    "How do we write our data and equations in terms of matrices in the first place? Suspending reality, suppose there is a line $\\beta_0 + \\beta_1 x = y$ that **does** pass through all three observations.  Then we'd solve\n",
    "\n",
    "\\begin{align*}\n",
    "  \\beta_0 + \\beta_1 = 2 \\\\\n",
    "  \\beta_0 + 2 \\beta_1 = 2 \\\\\n",
    "  \\beta_0 + 3 \\beta_1 = 4\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "for  $\\beta_0$ and  $\\beta_1$ where $\\beta_{0}$ and $\\beta_{1}$ are the intercept and slope of the desired line, respectively.  Let's write these equations in matrix form.  The left hand sides of the above equations can be written as\n",
    "<img src=\"images/LHS.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "while the right hand side is simply the vector\n",
    "\n",
    "\\begin{align*}\n",
    "  Y = \n",
    "  \\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    2 \\\\\n",
    "    4 \n",
    "  \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Thus we have the matrix equation $X \\beta = Y$ where\n",
    "\n",
    "\\begin{align}\n",
    "  X = \n",
    "  \\begin{bmatrix}\n",
    "    1 & 1\\\\\n",
    "    1 & 2\\\\\n",
    "    1 & 3\n",
    "  \\end{bmatrix}, \n",
    "  \\quad\n",
    "  \\beta = \n",
    "  \\begin{pmatrix}\n",
    "    \\beta_0 \\\\\n",
    "    \\beta_1 \n",
    "  \\end{pmatrix}, \n",
    "  \\quad \\mathrm{and} \\quad \n",
    "  Y = \n",
    "  \\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    2 \\\\\n",
    "    4 \n",
    "  \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "**This is a very important data format that we'll use throught the lab and the course**. Notice the dimensions of X and Y. Which dimension is observations, and which is features?\n",
    "\n",
    "\n",
    "However, becuase the data are not colinear, this equation doesn't have a perfect solution. To find the best possible solution we follow the process above, writing our error in terms of the betas: $error = \\sum (true - predicted)^2 = \n",
    "\\sum (Y - \\beta X)^2 = (Y - \\beta X)(Y - \\beta X)^T$. Then we take (matrix) derivatives and solve. We'll skip te ugly algebra.\n",
    "\n",
    "Ultimately, we get to this equation, named \"the normal equation\":\n",
    "\n",
    "\\begin{align}\n",
    "  X^T X \\beta = X^T Y.\n",
    "\\end{align}\n",
    "\n",
    "If $X^T X$ is invertible then the solution is\n",
    "\n",
    "\\begin{align}\n",
    "  \\beta = (X^T X)^{-1} X^T Y.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "Suppose the toy problem that we just considered included a second predictor variable, $\\beta_2$.\n",
    "\n",
    "* How would $X, \\beta$, and $Y$ change, if at all? What would the dimensions of each array be?\n",
    "* Would anything else change?\n",
    "\n",
    "Create a new markdown cell below and explain. Practice using LATEX and the $ flag to format your mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2: Building a model from scratch\n",
    "In this part, we will solve the equations for simple linear regression and find the best fit solution to our toy problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippets of code below implement the linear regression equations on the observed predictors and responses, which we'll call the training data set.  Let's walk through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed predictors\n",
    "x_train = np.array([1, 2, 3])\n",
    "y_train = np.array([2, 2, 4])\n",
    "\n",
    "# check dimensions \n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to be a proper 2D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 1)\n",
    "y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, compute means\n",
    "y_bar = np.mean(y_train)\n",
    "x_bar = np.mean(x_train)\n",
    "\n",
    "# build the two terms\n",
    "numerator = np.sum( (x_train - x_bar)*(y_train - y_bar) )\n",
    "denominator = np.sum((x_train - x_bar)**2)\n",
    "\n",
    "print(numerator.shape, denominator.shape) #check shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerator and denominator are scalars, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slope beta1\n",
    "beta_1 = numerator/denominator\n",
    "\n",
    "#intercept beta0\n",
    "beta_0 = y_bar - beta_1*x_bar\n",
    "\n",
    "print(\"The best-fit line is {0:8.6f} + {1:8.6f} * x\".format(beta_0, beta_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "Turn the code from the above cells into a function called `simple_linear_regression_fit`, that inputs the training data and returns `beta0` and `beta1`.\n",
    "\n",
    "To do this, copy and paste the code from the above cells below and adjust the code as needed, so that the training data becomes the input and the betas become the output.\n",
    "\n",
    "```python\n",
    "def simple_linear_regression_fit(x_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    return\n",
    "```\n",
    "\n",
    "Check your function by calling it with the training data from above and printing out the beta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1 ,2, 3])\n",
    "y_train = np.array([2, 2, 4])\n",
    "\n",
    "betas = simple_linear_regression_fit(x_train, y_train)\n",
    "\n",
    "beta_0 = betas[0]\n",
    "beta_1 = betas[1]\n",
    "\n",
    "print(\"The best-fit line is {0:8.6f} + {1:8.6f} * x\".format(beta_0, beta_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "* Plot the training data using a scatter plot.\n",
    "* Do the values of `beta0` and `beta1` seem reasonable?\n",
    "* Plot the best fit line with `beta0` and `beta1` together with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of `beta0` and `beta1` seem roughly reasonable.  They capture the positive correlation.  The line does appear to be trying to get as close as possible to all the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Building a model with `statsmodels` and `sklearn`\n",
    "\n",
    "Now that we can concretely fit the training data from scratch, let's learn two `python` packages to do it all for us:\n",
    "* [statsmodels](http://www.statsmodels.org/stable/regression.html) and \n",
    "* [scikit-learn (sklearn)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Our goal  is to show how to implement simple linear regression with these packages.  For an important sanity check, we compare the $\\beta$ values from `statsmodels` and `sklearn` to the $\\beta$ values that we found from above with our own implementation.\n",
    "\n",
    "For the purposes of this lab, `statsmodels` and `sklearn` do the same thing.  More generally though, `statsmodels` tends to be easier for inference \\[finding the values of the slope and intercept and dicussing uncertainty in those values\\], whereas `sklearn` has machine-learning algorithms and is better for prediction \\[guessing y values for a given x value\\]. (Note that both packages make the same guesses, it's just a question of which activity they provide more support for.\n",
    "\n",
    "**Note:** `statsmodels` and `sklearn` are different packages!  Unless we specify otherwise, you can use either one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for `statsmodels`.  `Statsmodels` does not by default include the column of ones in the $X$ matrix, so we include it manually with `sm.add_constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X matrix by appending a column of ones to x_train\n",
    "X = sm.add_constant(x_train)\n",
    "\n",
    "# this is the same matrix as in our scratch problem!\n",
    "print(X)\n",
    "\n",
    "# build the OLS model (ordinary least squares) from the training data\n",
    "toyregr_sm = sm.OLS(y_train, X)\n",
    "\n",
    "# do the fit and save regression info (parameters, etc) in results_sm\n",
    "results_sm = toyregr_sm.fit()\n",
    "\n",
    "# pull the beta parameters out from results_sm\n",
    "beta0_sm = results_sm.params[0]\n",
    "beta1_sm = results_sm.params[1]\n",
    "\n",
    "print(\"The regression coefficients from the statsmodels package are: beta_0 = {0:8.6f} and beta_1 = {1:8.6f}\".format(beta0_sm, beta1_sm))\n",
    "# print(\"(beta0, beta1) = (%f, %f)\" %(beta0_sm, beta1_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the beta parameters, `results_sm` contains a ton of other potentially useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn our attention to the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the least squares model\n",
    "toyregr_skl = linear_model.LinearRegression()\n",
    "\n",
    "# save regression info (parameters, etc) in results_skl\n",
    "results_skl = toyregr_skl.fit(x_train, y_train)\n",
    "\n",
    "# pull the beta parameters out from results_skl\n",
    "beta0_skl = toyregr_skl.intercept_\n",
    "beta1_skl = toyregr_skl.coef_[0]\n",
    "\n",
    "print(\"The regression coefficients from the sklearn package are: beta_0 = {0:8.6f} and beta_1 = {1:8.6f}\".format(beta0_skl, beta1_skl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should feel pretty good about ourselves now, and we're ready to move on to a real problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4:  The shape of things in `scikit-learn`\n",
    "Before diving right in to a \"real\" problem, we really ought to discuss more of the details of `sklearn`.  We do this now.  Along the way, we'll import the real-world dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scikit-learn` is the main `python` machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as `train_test_split`. It can be used in `python` by the incantation `import sklearn`.\n",
    "\n",
    "The library has a very well-defined interface. This makes the library a joy to use, and surely contributes to its popularity.  The API consists of three interfaces:\n",
    "1. estimator interface: builds and fits models\n",
    "   * This interface is at the core of the library\n",
    "   * Provides the `fit()` method\n",
    "   * Unsupervised and supervised learning algorithms can be accessed from the estimator interface\n",
    "2. predictor interface: makes predictions\n",
    "3. transformer interface: converts data\n",
    "\n",
    "You can refer to the original `scikit-learn` API paper for more details: [Buitinck, Lars, et al. \"API design for machine learning software: experiences from the scikit-learn project.\" arXiv preprint arXiv:1309.0238 (2013).]\n",
    "\n",
    "Let's see the structure of `scikit-learn` needed to make these fits. `.fit` always takes two arguments:\n",
    "```python\n",
    "  estimator.fit(Xtrain, ytrain)\n",
    "```\n",
    "We will consider two estimators in this lab: `LinearRegression` and `KNeighborsRegressor`.\n",
    "\n",
    "Critically, `Xtrain` must be in the form of an *array of arrays*, with the inner arrays each corresponding to one sample, and whose elements correspond to the feature values for that sample (visuals coming in a moment).\n",
    "\n",
    "`ytrain` on the other hand is a simple array of responses.  These are continuous for regression problems.\n",
    "\n",
    "![](images/sklearn2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice with `sklearn`\n",
    "We begin by loading up the `mtcars` dataset and cleaning it up a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load mtcars\n",
    "dfcars = pd.read_csv(\"data/mtcars.csv\")\n",
    "dfcars = dfcars.rename(columns={\"Unnamed: 0\":\"car name\"})\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split the dataset into a training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training set and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#set random_state to get the same split every time\n",
    "traindf, testdf = train_test_split(dfcars, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing set is around 20% of the total data; training set is around 80%\n",
    "print(\"Shape of full dataset is: {0}\".format(dfcars.shape))\n",
    "print(\"Shape of training dataset is: {0}\".format(traindf.shape))\n",
    "print(\"Shape of test dataset is: {0}\".format(testdf.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have training and test data.  We still need to select a predictor and a response from this dataset.  Keep in mind that we need to choose the predictor and response from both the training and test set.  You will do this in the exercises below.  However, we provide some starter code for you to get things going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the response variable that we're interested in\n",
    "y_train = traindf.mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shape of `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see the shape is to use the shape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is *not* an \"array of arrays\".  That's okay!  Remember, `sklearn` requires an array of arrays only for the predictor array!  You will have to pay close attention to this in the exercises later.\n",
    "\n",
    "For now, let's discuss two ways out of this debacle.  All we'll do is get `y_train` to be an array of arrays.  This doesn't hurt anything because `sklearn` doesn't care too much about the shape of `y_train`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reshape `y_train` to be an array of arrays using the `reshape` method.  We want the first dimension of `y_train` to be size $25$ and the second dimension to be size $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshape = y_train.values.reshape(y_train.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy.  Notice that `y_train.shape[0]` gives the size of the first dimension.\n",
    "\n",
    "Of course, there's an even easier way to get the correct shape right from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshape = traindf[['mpg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is a nice shortcut to reshaping an array.  `numpy` can infer a dimension based on the other dimensions specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshape = y_train.values.reshape(-1,1)\n",
    "y_train_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we said the second dimension should be size $1$.  Since the requirement of the `reshape()` method is that the requested dimensions be compatible, `numpy` decides the the first dimension must be size $25$.\n",
    "\n",
    "What would the `.shape` return if we did `y_train.values.reshape(-1,5)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, enough of that.  The whole reason we went through that whole process was to show you how to reshape your data into the correct format.\n",
    "\n",
    "**IMPORTANT:** Remember that your response variable `ytrain` can be a vector but your predictor variable `xtrain` ***must*** be an array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.4: Simple linear regression with automobile data\n",
    "We will now use `sklearn` to predict automobile mileage per gallon (mpg) and evaluate these predictions. We already loaded the data and split them into a training set and a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose the variables that we think will be good predictors for the dependent variable `mpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "* Pick one variable to use as a predictor for simple linear regression.  Create a markdown cell below and discuss your reasons.  \n",
    "* Justify your choice with some visualizations.  \n",
    "* Is there a second variable you'd like to use? For example, we're not doing multiple linear regression here, but if we were, is there another variable you'd like to include if we were using two predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "* Use `sklearn` to fit the training data using simple linear regression.\n",
    "* Use the model to make mpg predictions on the test set.  \n",
    "* Plot the data and the prediction.  \n",
    "* Print out the mean squared error for the training set and the test set and compare.\n",
    "\n",
    "**Hints:**\n",
    "* Use the following to perform the analysis:\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:  $k$-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we did a simple linear regression on the car data.\n",
    "\n",
    "Because we're good data scientists, we decide that a $k$ nearest-neighbor regression should also be performed.\n",
    "\n",
    "Now that you're familiar with the `sklearn` API, you're ready to do a KNN regression.  We can go a little quicker here.  Let's use $5$ nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knnreg = KNeighborsRegressor(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnreg.fit(X_train, y_train)\n",
    "r2 = knnreg.score(X_test, y_test)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "What is the $R^{2}$ score on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets vary the number of neighbors and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regdict = {}\n",
    "# Do a bunch of KNN regressions\n",
    "for k in [1, 2, 4, 6, 8, 10, 15]:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors=k)\n",
    "    knnreg.fit(X_train, y_train)\n",
    "    regdict[k] = knnreg # Store the regressors in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot it all\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(dfcars.wt, dfcars.mpg, 'o', label=\"data\")\n",
    "\n",
    "xgrid = np.linspace(np.min(dfcars.wt), np.max(dfcars.wt), 100)\n",
    "for k in [1, 2, 6, 10, 15]:\n",
    "    predictions = regdict[k].predict(xgrid.reshape(100,1))\n",
    "    if k in [1, 6, 15]:\n",
    "        ax.plot(xgrid, predictions, label=\"{}-NN\".format(k))\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the $1$-NN goes through every point on the training set but utterly fails elsewhere. Lets look at the scores on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 15) # Grid of k's\n",
    "scores_train = [] # R2 scores\n",
    "for k in ks:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors=k) # Create KNN model\n",
    "    knnreg.fit(X_train, y_train) # Fit the model to training data\n",
    "    score_train = knnreg.score(X_train, y_train) # Calculate R^2 score\n",
    "    scores_train.append(score_train)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "ax.plot(ks, scores_train,'o-')\n",
    "ax.set_xlabel(r'$k$')\n",
    "ax.set_ylabel(r'$R^{2}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we get a perfect $R^2$ at k=1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "* Make the same plot as above on the *test* set.\n",
    "* What is the best $k$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Check Answers:\n",
    "Answer these questions and see the bottom of the lab to check your answers.\n",
    "\n",
    "1. What is the major benefit of working in numpy?\n",
    "    - Numpy is much faster at mathematical operations than base python is.\n",
    "2. Why is it vital to use built-in numpy functions whenever possible?\n",
    "    - DIY activities in numpy occur at the Python level, rather than at the C level. They are categorically slower than using built-in functions.\n",
    "3. You need to find the running total of each element in a numpy array. For example, the input \\[2,5,3,2\\] should give the output \\[2,7,10,12\\]. Rather than writing your own code, think of at least two google querrys to find a relevant numpy function.\n",
    "    - Example 1: numpy sum array\n",
    "    - Example 2: numpy running sum\n",
    "4. What numpy function does the job above?\n",
    "    - the `cumsum` function"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

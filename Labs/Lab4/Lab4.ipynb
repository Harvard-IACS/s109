{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"iacs.png\"> S-109A Introduction to Data Science \n",
    "\n",
    "\n",
    "## Lab 4: Regularization and Cross Validation\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2018**<br>\n",
    "**Instructors:** Pavlos Protopapas and Kevin Rader<br>\n",
    "**Lab Instructors:** David Sondak and Will Claybaugh\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Run the cell below to properly highlight the exercises</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.exercise { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"<style>div.exercise { background-color: #ffcccc;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "<ol start=\"0\">\n",
    "  <li> Learning Goals </li>\n",
    "  <li> Review of regularized regression </li>\n",
    "  <li> Ridge regression with one predictor on a grid </li>\n",
    "  <li> Ridge regression with polynomial features on a grid</li>\n",
    "  <li> Cross-validation --- Multiple Estimates </li>\n",
    "  <li> Cross-validation --- Finding the best penalization parameter </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "In this lab, you will work with some noisy data.  You will use ridge regression and simple linear regression to fit linear, high-order monomial and polynomial features to the dataset.  You will attempt to figure out what degree polynomial fits the dataset the best and ultimately use cross validation to determine the best polynomial order.  Finally, you will automate the cross validation process using `sklearn` in order to determine the best regularization paramter for the ridge regression analysis on your dataset.\n",
    "\n",
    "By the end of this lab, you should:\n",
    "* Really understand regularized regression principles.\n",
    "* Have a good grasp of working with ridge regression through the `sklearn` API\n",
    "* Understand the effects of the regularization (a.k.a penalization) parameter on fits from ridge regression\n",
    "* Understand the ideas behind cross-validation\n",
    "  * Why is it necessary?\n",
    "  * Why is it important?\n",
    "  * Basic implementation details.\n",
    "* Be able to use `sklearn` objects to automate the cross validation process.\n",
    "\n",
    "**This lab corresponds to lectures 5 and 6 and maps on to homework 4 (and beyond).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn.apionly as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Review of regularized regression\n",
    "We briefly review the idea of regularization as introduced in lecture.  Recall that in the ordinary least squares problem we find the regression coefficients $\\boldsymbol{\\beta}\\in\\mathbb{R}^{m}$ that minimize the loss function \n",
    "\\begin{align*}\n",
    "  L(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\|y_i - \\boldsymbol{\\beta}^T \\mathbf{x}_i\\|^2.\n",
    "\\end{align*}\n",
    "Recall that we have $n$ observations.  Here $y_i$ is the response variable for observation $i$ and $\\mathbf{x}_i\\in\\mathbb{R}^{m}$ is a vector from the predictor matrix corresponding to observation $i$.\n",
    "\n",
    "The general idea behind regularization is to penalize the loss function to account for possibly very large values of the coefficients $\\boldsymbol{\\beta}$.  Instead of minimizing $L(\\boldsymbol{\\beta})$, we minimize the regularized loss function\n",
    "\\begin{align*}\n",
    "  L_{\\text{reg}}(\\boldsymbol{\\beta}) = L(\\boldsymbol{\\beta}) + \\lambda R(\\boldsymbol{\\beta})\n",
    "\\end{align*}\n",
    "where $R(\\boldsymbol{\\beta})$ is a penalty function and $\\lambda$ is a scalar that weighs the relative importance of this penalty.  In this lab we will explore one regularized regression model: `ridge` regression.  In ridge regression, the penalty function is the sum of the squares of the parameters, which is written as\n",
    "\\begin{align*}\n",
    "  L_{\\text{ridge}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\|y_i - \\boldsymbol{\\beta}^T \\mathbf{x}_i\\|^2 + \\lambda \\sum_{j=1}^m \\beta_{j}^{2}.\n",
    "\\end{align*}\n",
    "\n",
    "In lecture, you also learned about `LASSO` regression in which the penalty function is the sum of the absolute values of the parameters.  This is written as,\n",
    "\\begin{align*}\n",
    "  L_{\\text{LASSO}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\|y_i - \\boldsymbol{\\beta}^T \\mathbf{x}_i\\|^2 + \\lambda \\sum_{j=1}^m |\\beta_j|.\n",
    "\\end{align*}\n",
    "\n",
    "In this lab, we will show how these optimization problems can be solved with `sklearn` to determine the model parameters $\\boldsymbol{\\beta}$.  We will also show how to choose $\\lambda$ appropriately via cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "You will work with a synthetic dataset contained in `data/noisypopulation.csv`.  The data were generated from a specific function $f\\left(x\\right)$ (the actual form will not be revealed to you in this lab).  Noise was added to the function to generate synthetic, noisy observations via $y = f\\left(x\\right) + \\epsilon$ where $\\epsilon$ was drawn from a random distribution.  The idea here is that in real life the data you are working with often comes with noise.  Even if you could make observations at every single value of $x$, the true function may still be obscured.  Of course, the samples you actually take are usually a subset of all the possible observations.  In this lab, we will refer to observations at every single value of $x$ as the *population* and the subset of observations as *in-sample y* or simply *the observations*.\n",
    "\n",
    "The dataset contains three columns: \n",
    "1. `f` is the true function value \n",
    "2. `x` is the predictor\n",
    "3. `y` is the measured response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/noisypopulation.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will try out some regression methods to fit the data and see how well our model matches the true function `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert f, x, y to numpy array\n",
    "f = df.f.values\n",
    "x = df.x.values\n",
    "y = df.y.values\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the datset.  We will plot the true function value and the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(x, y, '.', alpha=0.8, label=r'Population')\n",
    "ax.plot(x, f, lw=4, label='True Response')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often the case that you just can't make observations at every single value of $x$.  We will simulate this situation by making a random choice of $60$ points from the full $200$ points.  We do it by choosing the indices randomly and then using these indices as a way of getting the appropriate samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=np.sort(np.random.choice(x.shape[0], size=60, replace=False)) # Using sort to make plotting easier later\n",
    "indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  If you are not familiar with the `numpy` `sort` method or the `numpy random.choice()` method, then please take a moment to look them up in the `numpy` documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, let's get the $60$ random samples from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe from the random points\n",
    "sample_df = pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes])) # New dataframe\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one more look at our data to see which points we've selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "ax.plot(sample_df['x'], sample_df['y'], 's', alpha=0.4, ms=10, label=\"in-sample y (observed)\")\n",
    "ax.plot(x,y, '.', label=r'Population $y$')\n",
    "ax.plot(x,f, lw=4, label='True Response')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do our favorite thing and split the sample data into training and testing sets.\n",
    "\n",
    "Note that here we are actually getting indices instead of the actual training and test set.  This is okay and is another way of generating train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datasize=sample_df.shape[0]\n",
    "\n",
    "#split dataset using the index, as we have x, f, and y that we want to split.\n",
    "itrain, itest = train_test_split(np.arange(60), train_size=0.8)\n",
    "\n",
    "xtrain = sample_df.x[itrain].values\n",
    "ftrain = sample_df.f[itrain].values\n",
    "ytrain = sample_df.y[itrain].values\n",
    "\n",
    "xtest= sample_df.x[itest].values\n",
    "ftest = sample_df.f[itest].values\n",
    "ytest = sample_df.y[itest].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  At this point we've explored our data a little bit, selected a sample of the dataset, and done a train-test split on the sample dataset.\n",
    "\n",
    "Let's move on to the data analysis.  We'll begin with ridge regression.  In particular we'll do ridge regression on a single predictor and compare it with simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's fit the old classic, linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit the model to training data\n",
    "simp_reg = LinearRegression().fit(xtrain.reshape(-1,1), ytrain)\n",
    "\n",
    "# save the beta coefficients\n",
    "beta0_sreg = simp_reg.intercept_\n",
    "beta1_sreg = simp_reg.coef_[0]\n",
    "\n",
    "print(\"(beta0, beta1) = ({0:8.6f}, {1:8.6f})\".format(beta0_sreg, beta1_sreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait! Unlike `statsmodels`, we don't get confidence intervals for the betas. Fortunately, we can bootstrap to build the confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 1</b></div>\n",
    "1. In the code below, two key steps of bootstrapping are missing. Fill in the code to draw sample indices with replacement and to fit the model to the bootstrap sample. You'll need `numpy`'s `np.random.choice`.  Here's the [function documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html) in case you need it.\n",
    "2. Visualize the results, and use `numpy`'s `np.percentile`: [function documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.percentile.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "bootstrap_beta1s = np.zeros(N)\n",
    "for cur_bootstrap_rep in range(N):\n",
    "    # use np.random.choice to select 48 indices ranging from 0 to 47, with replacement,\n",
    "    # and store them in inds_to_sample (48 is the size of our training set)\n",
    "    #your code here\n",
    "    \n",
    "    \n",
    "    # take the sample\n",
    "    x_train_resample = xtrain[inds_to_sample]\n",
    "    y_train_resample = ytrain[inds_to_sample]\n",
    "    \n",
    "    # fit the model\n",
    "    #your code here\n",
    "    \n",
    "    # extract the beta1 and store\n",
    "    bootstrap_beta1s[cur_bootstrap_rep] = bootstrap_model.coef_[0]\n",
    "    \n",
    "# visualize the results (ideally, include the 5th and 95th percentiles to give a 90% confidence interval, using\n",
    "# numpy's percentile() function)\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we find that the bootstrap $90\\%$ confidence interval is well away from $0$. We can confidently say that $\\beta_{1}$ is not secretly $0$ and we're being fooled by randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll dive into ridge regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ridge regression with one predictor on a grid\n",
    "\n",
    "To begin, we'll use `sklearn` to do simple linear regression on the sampled training data.  We'll then do ridge regression with the same data, setting the penalty parameter $\\lambda$ to zero.  Setting $\\lambda = 0$ reduces the ridge problem to the simple ordinary least squares problem, so we expect the results of these models to be identical. \n",
    "\n",
    "We will store the regression coefficients in a dataframe for easy comparison.  The cell below provides some code to set up the dataframe ahead of time.  Notice that we don't know the actual values in the `pandas` series, so we just set them to `NaN`.  We will overwrite these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regression_coeffs = dict() # Store regression coefficients from each model in a dictionary\n",
    "\n",
    "regression_coeffs['OLS'] = [np.nan]*2 # Initialize to 0\n",
    "regression_coeffs[r'Ridge $\\lambda = 0$'] = [np.nan]*2\n",
    "\n",
    "dfResults = pd.DataFrame(regression_coeffs) # Create dataframe\n",
    "\n",
    "dfResults.rename({0: r'$\\beta_{0}$', 1: r'$\\beta_{1}$'}, inplace=True) # Rename rows\n",
    "dfResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with simple linear regression to get the ball rolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simp_reg = LinearRegression() # build the the ordinary least squares model\n",
    "\n",
    "simp_reg.fit(xtrain.reshape(-1,1), ytrain) # fit the model to training data\n",
    "\n",
    "# save the beta coefficients\n",
    "beta0_sreg = simp_reg.intercept_\n",
    "beta1_sreg = simp_reg.coef_[0]\n",
    "\n",
    "dfResults['OLS'][:] = [beta0_sreg, beta1_sreg]\n",
    "dfResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = lambda x : beta0_sreg + beta1_sreg*x # make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above $\\boldsymbol\\beta$ coefficients as a benchmark for comparision to the ridge method.  The same coefficients can be obtained with ridge regression, which we demonstrate now.\n",
    "\n",
    "For reference, here is the ridge regression documentation: [sklearn.linear_model.Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippet of code below implements the ridge regression with $\\lambda = 0$.\n",
    "\n",
    "**Note:** The weight $\\lambda$ is referred to as `alpha` in the documentation.\n",
    "\n",
    "**Remark:** $\\lambda$ goes by many names including, but not limited to: regularization parameter, penalization parameter, shrinking parameter, and weight.  Regardless of these names, it is a hyperparameter.  That is, you set it before you begin the training process.  An algorithm can be very sensitive to its hyperparameters and we will discuss how a method for selecting the \"correct\" hyperparameter values later in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(alpha = 0) # build the ridge regression model with specified lambda, i.e. alpha\n",
    "\n",
    "ridge_reg.fit(xtrain.reshape(-1,1), ytrain) # fit the model to training data\n",
    "\n",
    "# save the beta coefficients\n",
    "beta0_ridge = ridge_reg.intercept_\n",
    "beta1_ridge = ridge_reg.coef_[0]\n",
    "\n",
    "ypredict_ridge = ridge_reg.predict(x.reshape(-1,1)) # make predictions everywhere\n",
    "\n",
    "dfResults[r'Ridge $\\lambda = 0$'][:] = [beta0_ridge, beta1_ridge]\n",
    "dfResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beta coefficients for linear and ridge regressions coincide for $\\lambda = 0$, as expected. We plot the data and fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(xtrain, ytrain, 's', alpha=0.3, ms=10, label=\"in-sample y (observed)\") # plot in-sample training data\n",
    "ax.plot(x, y, '.', alpha=0.4, label=\"population y\") # plot population data\n",
    "ax.plot(x, f, ls='-',  alpha=0.4, lw=4, label=\"True function\")\n",
    "ax.plot(x, y_predict(x), ls='--', lw=4, label=\"OLS\") # plot simple linear regression fit\n",
    "ax.plot(x, ypredict_ridge, ls='-.', lw = 4, label=\"Ridge\") # plot ridge regression fit\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "ax.legend(loc=4);\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2</b></div>\n",
    "Explore the effect of $\\lambda$ on ridge regression.\n",
    "\n",
    "Make a plot with of the ridge regression predictions with $\\lambda = 0, 5, 10, 100$.  Be sure to include a legend.\n",
    "\n",
    "What happens for very large $\\lambda$ (e.g. $\\lambda \\to \\infty$)?\n",
    "\n",
    "Your plot should look something like the following plot (doesn't have to be exact):\n",
    "![](ridge_lambda.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Recap\n",
    "That was nice, but we were just doing simple linear regression.  We really want to do more interesting regression problems like multilinear regression.  We will do so in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Ridge regression with polynomial features on a grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make a more complex model by adding polynomial features.  Instead of building the linear model $y = \\beta_0 + \\beta_1 x$, we build a polynomial model $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots \\beta_d x^d$ for some $d$ to be determined.  This regression will be linear though, since we'll be treating  $x^2, \\ldots, x^d$ themselves as predictors in the linear model.\n",
    "\n",
    "The design matrix $\\mathbf{X}$ contains columns corresponding to $1, x, x^2, \\ldots, x^d$. To build it, we use `sklearn`. (The particular design matrix is also known as the [*Vandermonde* matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix)).  For example, if we have three observations<br><br>\n",
    "\\begin{align*}\n",
    "  \\left\\{\\left(x_{1}, y_{1}\\right), \\left(x_{2}, y_{2}\\right), \\left(x_{3}, y_{3}\\right)\\right\\}\n",
    "\\end{align*}<br>\n",
    "and we want polynomial features up to and including degree $4$, the design matrix looks like<br><br>\n",
    "\\begin{align*}\n",
    "X = \\begin{bmatrix}\n",
    "x_1^0 & x_1^1 & x_1^2 & x_1^3 & x_1^4\\\\\n",
    "x_2^0 & x_2^1 & x_2^2 & x_2^3 & x_2^4\\\\\n",
    "x_3^0 & x_3^1 & x_3^2 & x_3^3 & x_3^4\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1& x_1^1 & x_1^2 & x_1^3 & x_1^4\\\\\n",
    "1 & x_2^1 & x_2^2 & x_2^3 & x_2^4\\\\\n",
    "1 & x_3^1 & x_3^2 & x_3^3 & x_3^4\\\\\n",
    "\\end{bmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3</b></div>\n",
    "1. Make a toy vector called `toy`, where  \n",
    "\\begin{align*}\n",
    "  \\mathrm{toy} = \n",
    "  \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    2 \\\\\n",
    "    5 \\\\\n",
    "  \\end{bmatrix}. \n",
    "\\end{align*}\n",
    "2. Build the feature matrix up to (and including) degree $4$. Confirm that the entries in the matrix are what you'd expect based on the above discussion.\n",
    "\n",
    "**Note:** You may use `sklearn` to build the matrix using `PolynomialFeatures()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now continue working with our data.  We write a function to make polynomial features of given degrees and we store the features in a dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4</b></div>\n",
    "\n",
    "The code provided below is missing a few lines and it's missing many comments.  Do the following:\n",
    "1. Comment every line of the code\n",
    "   * Normally, you won't do such excessive commenting.  In this case, we want to make sure you understand every single line since you didn't actually write this code.\n",
    "2. Fill in the missing lines\n",
    "   * Create a ridge regression object at each $\\lambda$ value in the list\n",
    "   * Perform the ridge regression using the `fit` method from the newly created ridge regression object\n",
    "   * Make a prediction on the grid and store the results in `ypredict_ridge`.\n",
    "\n",
    "**Note:** We're not giving you an example figure here since we gave you most of the code.\n",
    "\n",
    "**Warning!** Make sure you understand the entire code!  There are many nice things in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = 20 \n",
    "# You will create a grid of plots of this size (7 x 2)\n",
    "rows = 7\n",
    "cols = 2\n",
    "lambdas = [0., 1e-6, 1e-3, 1e-2, 1e-1, 1, 10] \n",
    "grid_to_predict = np.arange(0, 1, .01) \n",
    "\n",
    "Xtrain = PolynomialFeatures(d).fit_transform(xtrain.reshape(-1,1))\n",
    "test_set = PolynomialFeatures(d).fit_transform(grid_to_predict.reshape(-1,1))\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, sharex='col', figsize=(12, 24)) # Set up plotting objects\n",
    "\n",
    "for i, lam in enumerate(lambdas):\n",
    "    # your code here\n",
    "      # Create regression object\n",
    "      # Fit on regression object\n",
    "      # Do a prediction on the test set\n",
    "    \n",
    "    ### Provided code\n",
    "    axs[i,0].plot(xtrain, ytrain, 's', alpha=0.4, ms=10, label=\"in-sample y\")\n",
    "    axs[i,0].plot(grid_to_predict, ypredict_ridge, 'k-', label=r\"$\\lambda =  {0}$\".format(lam))\n",
    "    axs[i,0].set_ylabel('$y$')\n",
    "    axs[i,0].set_ylim((0, 1))\n",
    "    axs[i,0].set_xlim((0, 1))\n",
    "    axs[i,0].legend(loc='best')\n",
    "    \n",
    "    coef = ridge_reg.coef_.ravel()\n",
    "    \n",
    "    axs[i,1].semilogy(np.abs(coef), ls=' ', marker='o', label=r\"$\\lambda =  {0}$\".format(lam))\n",
    "    axs[i,1].set_ylim((1e-04, 1e+15))\n",
    "    axs[i,1].set_xlim(1, 20)\n",
    "    axs[i,1].yaxis.set_label_position(\"right\")\n",
    "    axs[i,1].set_ylabel(r'$\\left|\\beta_{j}\\right|$')\n",
    "    axs[i,1].legend(loc='best')\n",
    "\n",
    "axs[-1, 0].set_xlabel(\"x\")\n",
    "axs[-1, 1].set_xlabel(r\"$j$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, as we increase $\\lambda$ from 0 to 1, we start out overfitting, then doing well, and then our fits develop a mind of their own irrespective of data, as the penalty term dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 5</b></div>\n",
    "What would you expect if you compared a performance metric between these models on a grid?\n",
    "\n",
    "> **YOUR DISCUSSION HERE** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Recap\n",
    "We did a multilinear ridge regression on our dataset where the features were monomials.  We also assessed the impact of the regularization parameter on the solution.  \n",
    "\n",
    "Now we're going to go back to simple linear regression for the moment to illustrate some of the ideas behind cross validation.  In particular, we will use simple linear regression where our features are polynomials of a particular order.  We would like to determine what order monomial best fits our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:  Cross-validation --- Multiple Estimates\n",
    "Now we've done a ridge regression with polynomial features.  We've explored the effects of different penalization parameters on the solution.  The polynomial order and penalization parameter are called hyperparameters.  It would be nice to know what the best choice of $d$ and $\\lambda$ are.\n",
    "\n",
    "In this section, we will begin exploring how to find the best polynomial degree, but we'll do so in the context of simple linear regression.  This will naturally lead into a discussion of cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the fit\n",
    "First, we will create models using all polynomial features up to a maximum degree $d$.  We will compute the MSE for each of these models and use the minimum MSE on the test set as an indicator of which polynomial order is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 20 # Maximum monomial degree\n",
    "degrees = range(0,d+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate arrays for storing error\n",
    "error_train=np.empty(d+1)\n",
    "error_test=np.empty(d+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 6</b></div>\n",
    "* For each degree $d$, train on the training set and predict on the test set.\n",
    "* Store the training MSE in `error_train` and test MSE in `error_test`.\n",
    "\n",
    "**Note:** We have provided some starter code for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each degree, we now fit on the training set and predict on the test set\n",
    "# we accumulate the MSE on both sets in error_train and error_test\n",
    "for d in degrees: # for increasing polynomial degrees 0,1,2...\n",
    "    Xtrain = PolynomialFeatures(d).fit_transform(xtrain.reshape(-1,1))\n",
    "    Xtest = PolynomialFeatures(d).fit_transform(xtest.reshape(-1,1))\n",
    "\n",
    "    est = LinearRegression() # set up model\n",
    "    est.fit(Xtrain, ytrain) # fit\n",
    "    \n",
    "    # predict\n",
    "    # your code here\n",
    "    prediction_on_training = \n",
    "    prediction_on_test = \n",
    "    \n",
    "    # calculate mean squared error\n",
    "    error_train[d] = \n",
    "    error_test[d] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best degree is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestd = np.argmin(error_test)\n",
    "print(\"Best polynomial order is {0}.\".format(bestd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n",
    "plt.plot(degrees, error_test, marker='o', label='validation')\n",
    "plt.axvline(bestd, 0,0.5, color='k', label=\"min validation set error at d={0}\".format(bestd), alpha=0.3)\n",
    "plt.ylabel('mean squared error')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='best')\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the complexity-error plot.\n",
    "![m:caption](images/complexity-error-plot.png)\n",
    "Our plot resembles this pattern.  Notice that for $d$ larger than our value, we enter the overfitting regime.  The best $d$ is at the minimum of the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "At this point, if you're in or watching the lab, there is someone whose validation set told them that degree $10$ or $13$ or similar was the best model (it isn't). Can we do something to hedge against this risk? Is there a procedure that brings the class-wide results closer together, so that we each face less risk of picking a poor-but-lucky model?\n",
    "\n",
    "We could increase the size of the validation set, certainly, but that would take away from the data we have to train on. Instead, cross validation helps make more effecient use of the training data itself.\n",
    "\n",
    "The idea is illustrated in the figure below:\n",
    "\n",
    "![m:caption](images/train-cv2_simple.png)\n",
    "\n",
    "In fold 1, we fit each candidate model on the data in sections 1,2, and 3, and score each model on the unseen data in section 4 (highlighted). In the next fold, we fit each candidate model on sections 1,3, and 4, and score each model on the unseen data in section 2. \n",
    "\n",
    "At the end of the process, we have 4 estimates of how well each model will perform on unseen data. We can average these estimates to get an overall measure of how well each model will do on unseen data. We can also examine the spread of scores a given model recieved.\n",
    "\n",
    "Importantly, we kept the test set completely separate. Only one, final model is ever tested on the test set.\n",
    "\n",
    "**Technical notes**: \n",
    "1. The four cross-validation estimates are slightly correlated. As a result, there are diminishing returns to increasing the number of folds. (We might naively think that if averaging 4 estimates is good, averaging 50 estimates would be great.)\n",
    "2. We need the test set because the model that performed best in cross validation isn't quite as good as its average CV score would indicate (it might have gotten slightly lucky scores). To check, we score our final champion against the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-fold CV on our data set\n",
    "\n",
    "Let us now do 4-fold cross-validation on our  data set. We increase the complexity from degree 0 to degree 20. In each case we take the old training set, split it 4 ways into 4 folds, train on 3 folds, and calculate the validation error on the remaining one. We then average the erros over the four folds to get a cross-validation error for that $d$. Then we do what we did before: find the model with the lowest cross-validation error, and refit it using the entire training set. We can then use the test set to estimate $E_{out}$.\n",
    "\n",
    "We will use `KFold` from `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "n_folds = 4 \n",
    "kfold = KFold(n_folds)\n",
    "list(kfold.split(range(48)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong with the above? Why must we do the below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_folds, shuffle=True)\n",
    "list(kfold.split(range(48)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 7</b></div>\n",
    "In this exercise, you will carry out $4$-fold validation.  We have provided you with some skeleton code that does the following:\n",
    "* Loops over the folds\n",
    "* At each fold, generate the training and validation sets\n",
    "* At each fold, loop through the polynomial degrees\n",
    "* At each fold, and for each polynomial degree, call a function that returns the training and validation errors using the MSE.\n",
    "* Store the MSEs, for each degree and each fold, in `train_errors` and `valid_errors`.\n",
    "\n",
    "Here's what you need to do:\n",
    "* Write the function (called `compute_MSE`) that returns the training and validation error\n",
    "\n",
    "Your function should take in `Xtrain` and `Xvalid` at each value of `d`.  It should also accept `Ytrain` and `Yvalid` from the current fold.  The function must create a `LinearRegression` object, fit using that object, and predict on the training and validation using the result from the fit.  Then the function should calculate the MSE on both the training and validiation predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up problem size\n",
    "d = 20\n",
    "n_folds = 4\n",
    "degrees = range(d+1)\n",
    "train_errors = np.zeros((d+1, n_folds))\n",
    "valid_errors = np.zeros((d+1, n_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your function will go here\n",
    "def compute_MSE(Xtrain, Ytrain, Xvalid, Yvalid):\n",
    "    # Create LinearRegression object  \n",
    "    # Fit using the training data\n",
    "    # Make predictions using the training data and the validation data\n",
    "    # Compute MSE\n",
    "    \n",
    "    return train_MSE, valid_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to run things and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "fold = 0\n",
    "for train, valid in KFold(n_folds, shuffle=True).split(range(48)): # split data into train/test groups, 4 times\n",
    "    for d in degrees:\n",
    "        train_set = PolynomialFeatures(d).fit_transform(xtrain[train].reshape(-1,1))\n",
    "        valid_set = PolynomialFeatures(d).fit_transform(xtrain[valid].reshape(-1,1))\n",
    "        # Compute MSE at each degree and each fold\n",
    "        train_errors[d, fold], valid_errors[d, fold] = compute_MSE(train_set, ytrain[train], valid_set, ytrain[valid])\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our errors.  We average the MSEs over the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_errors = train_errors.mean(axis=1)\n",
    "mean_valid_errors = valid_errors.mean(axis=1)\n",
    "std_train_errors = train_errors.std(axis=1)\n",
    "std_valid_errors = valid_errors.std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can find the degree that minimizes the `cross-validation` error and, just like before, refit the model on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindeg = np.argmin(mean_valid_errors)\n",
    "print(\"The polynomial that best fits the data has degree {0}.\".format(mindeg))\n",
    "\n",
    "post_cv_train = PolynomialFeatures(mindeg).fit_transform(xtrain.reshape(-1,1)) # Training features from best polynomial\n",
    "post_cv_test = PolynomialFeatures(mindeg).fit_transform(xtest.reshape(-1,1)) # Test features from best polynomial\n",
    "\n",
    "# fit on whole training set now\n",
    "est = LinearRegression() # regression object\n",
    "est.fit(post_cv_train, ytrain) # fit\n",
    "\n",
    "pred_train = est.predict(post_cv_train) # predict on training set\n",
    "pred = est.predict(post_cv_test) # predict on test set\n",
    "\n",
    "errtr = mean_squared_error(ytrain, pred_train) # Train MSE\n",
    "err = mean_squared_error(pred, ytest) # Test MSE\n",
    "\n",
    "# Now make plots\n",
    "c0=sns.color_palette()[0]\n",
    "c1=sns.color_palette()[1]\n",
    "\n",
    "plt.plot(degrees, mean_train_errors, marker='o', label='CV Train error', alpha=0.9)\n",
    "plt.plot(degrees, mean_valid_errors, marker='^', label='CV Valid. error', alpha=0.9)\n",
    "\n",
    "# Try to represent the errors\n",
    "plt.fill_between(degrees, mean_valid_errors-std_valid_errors, mean_valid_errors+std_valid_errors, color=c1, alpha=0.2)\n",
    "\n",
    "plt.plot([mindeg], [err], 'o',  label='test set error')\n",
    "\n",
    "plt.ylabel('mean squared error')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='best')\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the cross-validation error minimizes at a low degree, and then increases. Because we have so few data points the spread in fold errors increases as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "That was a lot of stuff!  Basically, we went back to simple linear regression.  We didn't know what polynomial fit the data the best, so we tried various techniques to get the best polynomial degree.  This ultimately led us to the idea of cross validation.\n",
    "\n",
    "Now we will return to the ridge regularization problem that we were looking at before we got into that whole cross validation business.  Of course, we're pretty sure by now that a third degree polynomial will fit the data the best.\n",
    "\n",
    "However, we don't really have a good sense of what the penalization parameter $\\lambda$ should be in the Ridge regression case.  Let's do some cross validation to figure that out.\n",
    "\n",
    "Don't worry!  Things are actually going to be a lot less tedious this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cross-validation --- Finding the best penalization parameter\n",
    "Let's use cross-validation to determine the critical value of $\\lambda$, which we'll refer to as $\\lambda^*$. To do this we use the concept of a *meta-estimator* from `scikit-learn`.\n",
    "\n",
    "Model selection is supported by two distinct meta-estimators:\n",
    "1. `GridSearchCV`\n",
    "2. `RandomizedSearchCV`\n",
    "The input to these meta-estimators is an estimator, which has some hyperparameters (e.g. $\\lambda$) that need to be optimized, and a set of hyperparameter settings to search through.\n",
    "\n",
    "The concept of a meta-estimator allows us to wrap, for example, cross-validation, or methods that build and combine simpler models or schemes. For example:\n",
    "```python\n",
    "    est = Ridge()\n",
    "    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n",
    "    gridclassifier = GridSearchCV(est, param_grid=parameters, cv=4, scoring=\"neg_mean_squared_error\")\n",
    "```\n",
    "The `GridSearchCV` replaces the manual iteration over the folds using `KFolds` and the averaging we just did, doing it all for us. It takes a hyperparameter grid in the shape of a dictionary as input, and sets $\\lambda$ to the values you want to try, one by one. It then trains the model using cross-validation, and gets the error for each value of the hyperparameter $\\lambda$. Finally it compares the errors for the different $\\lambda$'s, and picks the best choice model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a helper function that we will use to get the best Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def cv_optimize_ridge(x: np.ndarray, y: np.ndarray, list_of_lambdas: list, n_folds: int =4):\n",
    "    est = Ridge()\n",
    "    parameters = {'alpha': list_of_lambdas}\n",
    "    # the scoring parameter below is the default one in ridge, but you can use a different one\n",
    "    # in the cross-validation phase if you want.\n",
    "    gs = GridSearchCV(est, param_grid=parameters, cv=n_folds, scoring=\"neg_mean_squared_error\")\n",
    "    gs.fit(x, y)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 8</b></div>\n",
    "Use the function above to fit the model on the training set with $4$-fold cross validation.  Save the fit as the variable `fitmodel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0]\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitmodel.best_estimator_, \"\\n\")\n",
    "print(fitmodel.best_params_, \"\\n\")\n",
    "print(fitmodel.best_score_, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also output the mean cross-validation error at different $\\lambda$ (with a negative sign, as scikit-learn likes to maximize negative error which is equivalent to minimizing error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitmodel.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_lambdas = [d['alpha'] for d in fitmodel.cv_results_['params']]\n",
    "fit_scores = fitmodel.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a `log-log` plot of `-fit_scores` versus `fit_lambdas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.plot(fit_lambdas, -fit_scores, ls=' ', marker='o')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('$\\lambda$')\n",
    "ax.set_ylabel('scores');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SK-learn's `cross_val_score`: Easier Cross Validation\n",
    "`GridSearchCV` is an important tool when you are searching over many hyperparameters (and believe us, you will be), but when you only need to get CV scores for a particular model, some students find `cross_val_score` more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr_object =  Ridge(alpha=0)\n",
    "cross_val_score(lr_object, Xtrain, ytrain, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over particular models and get scores for each (equivalent to GridSearchCV over the given parameter settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_alpha in [1e-8, 1e-4, 1e-2, 1.0, 10.0]:\n",
    "    lr_object =  Ridge(alpha=cur_alpha)\n",
    "    scores = cross_val_score(lr_object, Xtrain, ytrain, cv=5)\n",
    "    print(\"lambda {0}\\t R^2 scores: {1}\\t Mean R^2: {2}\".format(cur_alpha,scores,np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Cross Validation: `RidgeCV` and `LassoCV`\n",
    "Some sklearn models have built-in, automated cross validation to tune their hyper parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridgeCV_object = RidgeCV(alphas=(1e-8, 1e-4, 1e-2, 1.0, 10.0), cv=5)\n",
    "ridgeCV_object.fit(Xtrain, ytrain)\n",
    "print(\"Best model searched:\\nalpha = {}\\nintercept = {}\\nbetas = {}, \".format(ridgeCV_object.alpha_,\n",
    "                                                                            ridgeCV_object.intercept_,\n",
    "                                                                            ridgeCV_object.coef_\n",
    "                                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note: \n",
    "\n",
    "1. For any tool more automated than literally using k_fold, just setting `cv=5` will **NOT** shuffle your data by default.\n",
    "2. To force shuffling, explicitly pass a `KFold` object (with shuffling turned on) to the cv argument\n",
    "3. You may prefer a strategy where you shuffle the rows of your data at the outset of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare and pass a KFold object to properly shuffle the training data, and/or set the random state\n",
    "splitter = KFold(5, random_state=42, shuffle=True)\n",
    "\n",
    "ridgeCV_object = RidgeCV(alphas=(1e-8, 1e-4, 1e-2, 1.0, 10.0), cv=splitter)\n",
    "ridgeCV_object.fit(Xtrain, ytrain)\n",
    "print(\"Best model searched:\\nalpha = {}\\nintercept = {}\\nbetas = {}, \".format(ridgeCV_object.alpha_,\n",
    "                                                                            ridgeCV_object.intercept_,\n",
    "                                                                            ridgeCV_object.coef_\n",
    "                                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Refitting on full training set\n",
    "At this point, we have determined the best penalization parameter for the ridge regression on our current dataset using cross validation.  Let's refit the estimator on the training set and calculate and plot the test set error and the polynomial coefficients. Notice how many of these coefficients have been pushed to lower values or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 9</b></div>\n",
    "Assign to variable `est` the classifier obtained by fitting the entire training set using the best $\\lambda$ found above.  Assign the predictions to the variable `ypredict_ridge_best`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the final plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "left = 0\n",
    "right = 1\n",
    "\n",
    "axs[left].plot(x,f, lw=4, label='True Response')\n",
    "axs[left].plot(xtrain, ytrain, 's', alpha=0.3, ms=10, label=\"in-sample y (observed)\")\n",
    "axs[left].plot(x, y, '.', alpha=0.8, label=\"population y\")\n",
    "axs[left].plot(grid_to_predict, ypredict_ridge_best, 'k--', label=r\"$\\lambda =  {{{0:1.4f}}}$\".format(best_lambda))\n",
    "axs[left].set_ylabel('$y$')\n",
    "axs[left].set_ylim((0, 1))\n",
    "axs[left].set_xlim((0, 1))\n",
    "axs[left].legend(loc=2)\n",
    "coef = est.coef_.ravel()\n",
    "axs[right].semilogy(np.abs(coef), marker='o', label=r\"$\\lambda =  {0}$\".format(best_lambda))\n",
    "axs[right].set_ylim((1e-04, 1.0e+11))\n",
    "axs[right].set_xlim(1, 20)\n",
    "axs[right].yaxis.set_label_position(\"right\")\n",
    "axs[right].set_ylabel(r'$\\left|\\beta_{j}\\right|$')\n",
    "axs[right].legend(loc='best')\n",
    "axs[left].set_xlabel(\"x\")\n",
    "axs[right].set_xlabel(r'$j$');"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
